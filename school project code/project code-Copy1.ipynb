{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.graphics.tsaplots as smt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Combined_News_DJIA.csv\",low_memory=False,\n",
    "                    parse_dates=[0])\n",
    "\n",
    "full_stock = pd.read_csv(\"DJIA_table.csv\",low_memory=False,\n",
    "                    parse_dates=[0])\n",
    "\n",
    "#add the closing stock value to the df - this will be the y variable\n",
    "df[\"Close\"]=full_stock.Close\n",
    "df[\"Open\"]=full_stock.Open\n",
    "#df[\"Low\"]=full_stock.Close\n",
    "\n",
    "#show how the dataset looks like\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'][1988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new label(sentiment) based on news impact on stock prices\n",
    "def news_impact_label():\n",
    "    news_impact = []\n",
    "    for i in range(len(df['Close'])):\n",
    "        if i != 0:\n",
    "            if df['Close'][i-1] < df['Close'][i]:\n",
    "                news_impact.append(1)\n",
    "            elif df['Close'][i-1] > df['Close'][i]:\n",
    "                news_impact.append(0)\n",
    "            elif df['Close'][i-1] == df['Close'][i]:\n",
    "                news_impact.append(0)\n",
    "    news_impact.append(0)\n",
    "    return news_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['News_Impact'] = news_impact_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the label column\n",
    "df = df.drop([\"Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for NAN\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, ' ', regex=True)\n",
    "\n",
    "#sanity check\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anakin = SentimentIntensityAnalyzer()\n",
    "\n",
    "Anakin.polarity_scores(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "detect_subjectivity(\" \") #should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the headline columns' names\n",
    "cols = []\n",
    "for i in range(1,26):\n",
    "    col = (\"Top{}\".format(i))\n",
    "    cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vect=time.time()\n",
    "print(\"ANAKIN: 'Intializing the process..'\")\n",
    "\n",
    "#get the name of the headline columns\n",
    "cols = []\n",
    "for i in range(1,26):\n",
    "    col = (\"Top{}\".format(i))\n",
    "    cols.append(col)\n",
    "\n",
    "\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype(str) # Make sure data is treated as a string\n",
    "    df[col+'_comp']= df[col].apply(lambda x:Anakin.polarity_scores(x)['compound'])\n",
    "    df[col+'_sub'] = df[col].apply(detect_subjectivity)\n",
    "    print(\"{} Done\".format(col))\n",
    "    \n",
    "print(\"VADER: Vaderization completed after %0.2f Minutes\"%((time.time() - start_vect)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the text isn't required anymore\n",
    "df = df.drop(cols,axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_cols = []\n",
    "for col in cols:\n",
    "    comp_col = col + \"_comp\"\n",
    "    comp_cols.append(comp_col)\n",
    "\n",
    "w = np.arange(1,26,1).tolist()\n",
    "w.reverse()\n",
    "\n",
    "weighted_comp = []\n",
    "max_comp = []\n",
    "min_comp = []\n",
    "for i in range(0,len(df)):\n",
    "    a = df.loc[i,comp_cols].tolist()\n",
    "    weighted_comp.append(np.average(a, weights=w))\n",
    "    max_comp.append(max(a))\n",
    "    min_comp.append(min(a))\n",
    "\n",
    "df['compound_mean'] = weighted_comp\n",
    "df['compound_max'] = max_comp\n",
    "df['compound_min'] = min_comp\n",
    "\n",
    "\n",
    "sub_cols = []\n",
    "for col in cols:\n",
    "    sub_col = col + \"_sub\"\n",
    "    sub_cols.append(sub_col)\n",
    "\n",
    "\n",
    "weighted_sub = []\n",
    "max_sub = []\n",
    "min_sub = []\n",
    "for i in range(0,len(df)):\n",
    "    a = df.loc[i,sub_cols].tolist()\n",
    "    weighted_sub.append(np.average(a, weights=w))\n",
    "    max_sub.append(max(a))\n",
    "    min_sub.append(min(a))\n",
    "\n",
    "df['subjectivity_mean'] = weighted_sub\n",
    "df['subjectivity_max'] = max_sub\n",
    "df['subjectivity_min'] = min_sub\n",
    "\n",
    "to_drop = sub_cols+comp_cols\n",
    "df = df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(avg_len, col, df):\n",
    "    list_of_avg = []\n",
    "    list_len = [0 for x in range(avg_len)]\n",
    "    for i in range(len(df[col])):\n",
    "        if i > avg_len:\n",
    "            a = []\n",
    "            for val in list_len:\n",
    "                d = df[col][i - val] \n",
    "                a.append(d)\n",
    "            print(a)\n",
    "            avg = sum(a)/avg_len\n",
    "            list_len.append(avg)\n",
    "    df_name = 'avg '+str(avg_len)\n",
    "    print('list len: ', len(list_len))\n",
    "    print('df len: ', len(df[col]))\n",
    "    df[df_name] = list_len\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i = 0\n",
    "while i <= df.Close.count():\n",
    "    if i <= df.Close.count()-4:\n",
    "        new_df = np.append(new_df, [[df['Open'][i],df['Open'][i-1],df['Open'][i-2],df['Open'][i-3]]\n",
    "        ], axis=0)\n",
    "        \n",
    "    if i <= df.Open.count()-5:\n",
    "        target_df = np.append(target_df, [[df['Open'][i+4]]], axis=0)\n",
    "    print(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg(5, 'Close', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_len = [x for x in range(10)]\n",
    "list_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=df.Date, y=df.Close,\n",
    "                    mode='lines'))\n",
    "title = []\n",
    "title.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='Development of stock values from Aug, 2008 to Jun, 2016',\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))\n",
    "fig1.update_layout(xaxis_title='Date',\n",
    "                   yaxis_title='Closing stock value (in $)',\n",
    "                  annotations=title)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for quick plotting and testing of stationarity\n",
    "def stationary_plot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
    "    \"\"\"\n",
    "        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n",
    "        \n",
    "        y - timeseries\n",
    "        lags - how many lags to include in ACF, PACF calculation\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_plot(df.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df.Close - df.Close.shift(7)\n",
    "stationary_plot(diff[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = diff - diff.shift(1)\n",
    "stationary_plot(diff2[7+1:], lags=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=df.Date, y=df.compound_mean,\n",
    "                    mode='lines',\n",
    "                    name='Mean'))\n",
    "fig2.add_trace(go.Scatter(x=df.Date, y=df.compound_max,\n",
    "                    mode='lines',\n",
    "                    name='Maximum'))\n",
    "fig2.add_trace(go.Scatter(x=df.Date, y=df.compound_min,\n",
    "                    mode='lines',\n",
    "                    name='Minimum'))\n",
    "title = []\n",
    "title.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='Development of sentiment compound score',\n",
    "                               font=dict(family='Arial',\n",
    "                                       size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))\n",
    "fig2.update_layout(xaxis_title='Date',\n",
    "                   yaxis_title='Compound score',\n",
    "                  annotations=title)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compm_hist = px.histogram(df, x=\"compound_mean\")\n",
    "compm_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_mean,\n",
    "                    mode='lines',\n",
    "                    name='Mean'))\n",
    "fig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_min,\n",
    "                    mode='lines',\n",
    "                    name='Min'))\n",
    "fig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_max,\n",
    "                    mode='lines',\n",
    "                    name='Max'))\n",
    "title = []\n",
    "title.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='Development of subjectivity score',\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False))\n",
    "fig3.update_layout(xaxis_title='Date',\n",
    "                   yaxis_title='Subjectivity score',\n",
    "                  annotations=title)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_hist = px.histogram(df, x=\"subjectivity_mean\")\n",
    "subm_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_ratio (col):\n",
    "    return len(np.unique(col))/len(col)\n",
    "\n",
    "cols = ['Close', 'News_Impact', 'compound_mean', 'compound_max', 'compound_min', 'subjectivity_mean', 'subjectivity_max', 'subjectivity_min']\n",
    "\n",
    "ur = []\n",
    "var = []\n",
    "for col in cols:\n",
    "    ur.append(unique_ratio(df[col]))\n",
    "    var.append(np.var(df[col]))\n",
    "    \n",
    "feature_sel = pd.DataFrame({'Column': cols, \n",
    "              'Unique': ur,\n",
    "              'Variance': var})\n",
    "feature_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_fig = go.Figure(data=go.Scatter(\n",
    "    x=feature_sel.Column,\n",
    "    y=feature_sel.Unique,\n",
    "    mode='markers',\n",
    "    marker=dict(size=(feature_sel.Unique*100)),\n",
    "))\n",
    "sel_fig.update_layout(title='Ratio of unique values', \n",
    "                      yaxis_title='Unique ratio')\n",
    "sel_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['subjectivity_min', 'subjectivity_max']\n",
    "clean_df = df.drop(drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag the extracted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = clean_df.copy()\n",
    "lag_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_lag = list(lag_df.columns)\n",
    "to_lag_4 = to_lag[1]\n",
    "to_lag_1 = to_lag[2:len(to_lag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lagging text features two days back\n",
    "for col in to_lag_1:\n",
    "    for i in range(1,3):\n",
    "        new_name = col + ('_lag_{}'.format(i))\n",
    "        lag_df[new_name] = lag_df[col].shift(i)\n",
    "    \n",
    "#lagging closing values 4 days back\n",
    "for i in range(1, 5):\n",
    "    new_name = to_lag_4 + ('_lag_{}'.format(i))\n",
    "    lag_df[new_name] = lag_df[to_lag_4].shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show many rows need to be removed\n",
    "lag_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = lag_df.drop(lag_df.index[[np.arange(0,4)]])\n",
    "lag_df = lag_df.reset_index(drop=True)\n",
    "\n",
    "#sanity check for NaNs\n",
    "lag_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time-series cross-validation set 10 folds \n",
    "tscv = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "scaler = StandardScaler()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lag_df.drop(['News_Impact'],axis=1)\n",
    "#X = lag_df.drop(['Label'],axis=1)\n",
    "#X.index = X[\"Date\"]\n",
    "X = X.drop(['Date'],axis=1)\n",
    "y = lag_df.News_Impact\n",
    "\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#sanity check  hahaha\n",
    "(len(X_train)+len(X_test))==len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_df = int((75/100)*len(df))\n",
    "#train_df = df[:len_df]\n",
    "#test_df = df[len_df:]\n",
    "train_df = X_train.copy()\n",
    "train_df['News_Impact'] = y_train.values\n",
    "#train_df['Label'] = y_train.values\n",
    "test_df = X_test.copy()\n",
    "test_df['News_Impact'] = y_test.values\n",
    "#test_df['Label'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff = df.drop(columns = ['Date', 'News_Impact'])\n",
    "#dff_i = df[['News_Impact']]\n",
    "#dff = dff.merge(dff_i, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff['News_Impact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "# Tensor transform\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# SVHN training datasets\n",
    "#svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
    "\n",
    "batch_size = 21\n",
    "test_batch_size = 399\n",
    "num_workers = 0\n",
    "\n",
    "# build DataLoaders for SVHN dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_df.values, #dff.values,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_df.values, #dff.values,\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_loader))#[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    print(idx)\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This funcn creates batches\n",
    "def perfect_batches2(df, batch_size):\n",
    "    batched_data = []\n",
    "    if len(df)%batch_size != 0:\n",
    "        print('Cannot create a perfect batch size with the input. \\nTry the following: ')\n",
    "        for i in range(len(df)):\n",
    "            if i != 0 and len(df)%i == 0:\n",
    "                print('This number can do: ', i)\n",
    "        print('\\n\\nEnd of numbers!')\n",
    "        return None\n",
    "    i = 0\n",
    "    while i <= len(df)-batch_size+1:\n",
    "        batch = []\n",
    "        j = 0\n",
    "        while j <= batch_size:\n",
    "            #print('debug mode: ', df.values[i]) #This line is just for debugging\n",
    "            #batch.append(torch.tensor(df.values[i])) #original\n",
    "            batch.append(df.values[i]) #new --modified\n",
    "            i += 1\n",
    "            j += 1\n",
    "        #batched_data.append(torch.stack(batch)) #original\n",
    "        batched_data.append(np.stack(batch)) #new --modified\n",
    "    #return torch.stack(batched_data).float()\n",
    "    return np.stack(batched_data)#.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batched_train = perfect_batches2(train_df, 5)\n",
    "batched_test = perfect_batches2(test_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###if above is going to give problems use Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_len, 200)\n",
    "        self.bn1 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.bn3 = nn.BatchNorm1d(50)\n",
    "        self.fc4 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.double()\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        #x = self.tanh(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def modelAndOptim(input_len, lr):\n",
    "    #modell = movieModel(input_len)\n",
    "    modell = BinaryClassifier(input_len)\n",
    "    optimizer = optim.SGD(modell.parameters(), lr)\n",
    "    #optimizer = optim.Adam(modell.parameters(), lr)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    #loss_func = nn.BCELoss()\n",
    "    #loss_func = nn.CrossEntropyLoss()\n",
    "    return modell, optimizer, loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, actual):\n",
    "    pred = torch.tensor([1.0 if i >= 0.5 else 0.0 for i in pred])\n",
    "    #print('preddd: ', pred)\n",
    "    #actual = torch.tensor([1 if i >= 7.0 else 0 for i in actual])\n",
    "    same = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == actual[i]:\n",
    "            same.append(1)\n",
    "        else:\n",
    "            same.append(0)\n",
    "    same = torch.tensor(same)\n",
    "    #print(pred)\n",
    "    #print(actual)\n",
    "    #print(same)\n",
    "    return (sum(same)/len(same)).item()\n",
    "    \n",
    "#accuracy(v, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a train method and validation\n",
    "#Theres an error i spotted i have to make sure that it loops through the batch one at a time not all at once\n",
    "model, optimizer, loss = modelAndOptim(22, 0.005)\n",
    "def train(epoch, train_data, model, optimizer, loss):\n",
    "    model.train()\n",
    "    model = model.double()\n",
    "    sum_loss = []\n",
    "    accuracy_count = []\n",
    "    for e in range(epoch):\n",
    "        losses = []\n",
    "        #acc = []\n",
    "        acc = None\n",
    "        \n",
    "        #train_data = next(iter(train_data)).type(torch.float32)\n",
    "        #print('train_data')\n",
    "        #print(train_data)\n",
    "        #print(train_data[0, :-1])\n",
    "        #print(train_data[0, :])\n",
    "        for batch in train_data:\n",
    "            #for rec in batch:\n",
    "            features = batch[:, :-1]\n",
    "            #print(features)\n",
    "            label = batch[:, -1:]\n",
    "            #print(label)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted = model(features)\n",
    "            #print('predited')\n",
    "            #print('p: ', len(predicted))\n",
    "            #print('l: ', label)\n",
    "            predicted = predicted.view(len(predicted), 1)\n",
    "            label = label.view(len(label), 1)\n",
    "            #print('pridicted size: ', predicted.shape)\n",
    "            #print('label dType: ', label.shape)\n",
    "            #print('pridicted: ', predicted)\n",
    "            #print('label: ', label)\n",
    "            loss_error = loss(predicted.double(), label.double())\n",
    "            losses.append(loss_error.item())\n",
    "            loss_error.backward()\n",
    "            optimizer.step()\n",
    "            #acc.append(accuracy(predicted.view(-1), label.view(-1)))\n",
    "            acc = accuracy(predicted.view(-1), label.view(-1))\n",
    "            #print('Average error at current  ', sum(losses)/len(losses))\n",
    "            #print('done')\n",
    "        #print(predicted)\n",
    "        print('Average error at ',e,' epoch:  ', sum(losses)/len(losses))\n",
    "        #acr = sum(acc)/len(acc)\n",
    "        acr = acc\n",
    "        print('current batch accurate prediction: ', acr)\n",
    "        accuracy_count.append(acr)\n",
    "        sum_loss.append(sum(losses)/len(losses))\n",
    "        if acr >= 0.999 and e == 500:\n",
    "            plt.plot(sum_loss, label = 'loss')\n",
    "            #print(sum_loss)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            return predicted, label\n",
    "    print('\\nOverall Accuracy: ', sum(accuracy_count)/len(accuracy_count))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    #plt.plot(label, label = 'True')\n",
    "    plt.plot(sum_loss, label = 'loss')\n",
    "    #print(sum_loss)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('end of train')\n",
    "    print(\"\")\n",
    "    return predicted, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pr, lr = train(250, train_loader, model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the ANN architecture\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.double()\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "model = SentimentModel(input_size = 22, hidden_size=100, output_size=1)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model = model.double()\n",
    "    acr = []\n",
    "    sum_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        accuracy_count = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #print(data)\n",
    "            #inputs, labels = data\n",
    "            features = data[:, :-1]\n",
    "            labels = data[:, -1:]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            acc = accuracy(outputs.view(-1), labels.view(-1))\n",
    "        print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "        acr = acc\n",
    "        print('Accuracy: ', acr)\n",
    "        accuracy_count.append(acr)\n",
    "    sum_loss.append(sum(losses)/len(losses))\n",
    "    if acr >= 0.999:\n",
    "        plt.plot(sum_loss, label = 'loss')\n",
    "        #print(sum_loss)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return outputs, labels\n",
    "train(model, train_loader, criterion, optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_data, model, optimizer, loss):\n",
    "    model.eval()\n",
    "    #model = model.float()\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    accuracy_count = []\n",
    "    for e in range(epoch):\n",
    "        losses = []\n",
    "        #print(len(test_data))\n",
    "        \n",
    "        #train_data = next(iter(train_data)).type(torch.float32)\n",
    "        #print('train_data')\n",
    "        #print(train_data)\n",
    "        #print(train_data[0, :-1])\n",
    "        #print(train_data[0, :])\n",
    "        for batch in test_data:\n",
    "            features = batch[:, :-1]\n",
    "            #print(features)\n",
    "            label = batch[:, -1]\n",
    "            #print(features.shape)\n",
    "            #features is #7, label is #1\n",
    "\n",
    "            predicted = model(features)\n",
    "            #print(predicted)\n",
    "            predicted = predicted.view(len(predicted), 1)\n",
    "            pred_labels.append(predicted)\n",
    "            label = label.view(len(predicted), 1)\n",
    "            actual_labels.append(label)\n",
    "            #print('pridicted size: ', predicted.shape)\n",
    "            #print('label dType: ', label.shape)\n",
    "            #print('pridicted: ', predicted)\n",
    "            #print('label: ', label)\n",
    "            loss_error = loss(predicted, label)\n",
    "            losses.append(loss_error.item())\n",
    "            acc = accuracy(predicted.view(-1), label.view(-1))\n",
    "            print(':::percentage of correct batch prediction:- ', acc*100, '%')\n",
    "            accuracy_count.append(acc)\n",
    "            #loss_error.backward()\n",
    "            #optimizer.step()\n",
    "            #print('done')\n",
    "        #print(predicted)\n",
    "        #print('Average error at ',e,' epoch:  ', sum(losses)/len(losses))\n",
    "    print('\\nOverall Accuracy for validation: ', sum(accuracy_count)/len(accuracy_count))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print('end of test')\n",
    "    print(\"\")\n",
    "    return pred_labels, actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p, l = train(500, test_loader, model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t, l_t = test(1, test_loader, model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(pred, actual):\n",
    "    #print(pred)\n",
    "    pred = torch.tensor([1.0 if i >= 0.5 else 0.0 for i in pred])\n",
    "    #print('preddd: ', pred)\n",
    "    #actual = torch.tensor([1 if i >= 7.0 else 0 for i in actual])\n",
    "    same = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == actual[i]:\n",
    "            same.append(1)\n",
    "        else:\n",
    "            same.append(0)\n",
    "    same = torch.tensor(same)\n",
    "    error = 0\n",
    "    for i in same:\n",
    "        if i == 0:\n",
    "            error += 1\n",
    "    #print(pred)\n",
    "    #print(actual)\n",
    "    print(same)\n",
    "    print('out of ', len(actual), ' predicted values, the miss predicted values are: ', error)\n",
    "    print('Thus making the accuracy: ', (sum(same)/len(same)).item())\n",
    "    print('Where Zero appears is the data that was miss predicted')\n",
    "    return pred, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, a = comp(p_t[0], l_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(a,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(a, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
