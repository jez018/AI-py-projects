{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>302.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>...</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Color</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>602.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>994.0</td>\n",
       "      <td>English</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.35</td>\n",
       "      <td>85000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Color</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>813.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doug Walker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rob Walker</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>Color</td>\n",
       "      <td>Scott Smith</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>Daphne Zuniga</td>\n",
       "      <td>637.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>Color</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.0</td>\n",
       "      <td>Valorie Curry</td>\n",
       "      <td>841.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crime|Drama|Mystery|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>359.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>593.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>16.00</td>\n",
       "      <td>32000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>Color</td>\n",
       "      <td>Benjamin Roberds</td>\n",
       "      <td>13.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maxwell Moody</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drama|Horror|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5041</th>\n",
       "      <td>Color</td>\n",
       "      <td>Daniel Hsia</td>\n",
       "      <td>14.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>Daniel Henney</td>\n",
       "      <td>946.0</td>\n",
       "      <td>10443.0</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.35</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>Color</td>\n",
       "      <td>Jon Gunn</td>\n",
       "      <td>43.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Brian Herzlinger</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85222.0</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.85</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5043 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      color      director_name  num_critic_for_reviews  duration  \\\n",
       "0     Color      James Cameron                   723.0     178.0   \n",
       "1     Color     Gore Verbinski                   302.0     169.0   \n",
       "2     Color         Sam Mendes                   602.0     148.0   \n",
       "3     Color  Christopher Nolan                   813.0     164.0   \n",
       "4       NaN        Doug Walker                     NaN       NaN   \n",
       "...     ...                ...                     ...       ...   \n",
       "5038  Color        Scott Smith                     1.0      87.0   \n",
       "5039  Color                NaN                    43.0      43.0   \n",
       "5040  Color   Benjamin Roberds                    13.0      76.0   \n",
       "5041  Color        Daniel Hsia                    14.0     100.0   \n",
       "5042  Color           Jon Gunn                    43.0      90.0   \n",
       "\n",
       "      director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "0                         0.0                   855.0  Joel David Moore   \n",
       "1                       563.0                  1000.0     Orlando Bloom   \n",
       "2                         0.0                   161.0      Rory Kinnear   \n",
       "3                     22000.0                 23000.0    Christian Bale   \n",
       "4                       131.0                     NaN        Rob Walker   \n",
       "...                       ...                     ...               ...   \n",
       "5038                      2.0                   318.0     Daphne Zuniga   \n",
       "5039                      NaN                   319.0     Valorie Curry   \n",
       "5040                      0.0                     0.0     Maxwell Moody   \n",
       "5041                      0.0                   489.0     Daniel Henney   \n",
       "5042                     16.0                    16.0  Brian Herzlinger   \n",
       "\n",
       "      actor_1_facebook_likes        gross                           genres  \\\n",
       "0                     1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
       "1                    40000.0  309404152.0         Action|Adventure|Fantasy   \n",
       "2                    11000.0  200074175.0        Action|Adventure|Thriller   \n",
       "3                    27000.0  448130642.0                  Action|Thriller   \n",
       "4                      131.0          NaN                      Documentary   \n",
       "...                      ...          ...                              ...   \n",
       "5038                   637.0          NaN                     Comedy|Drama   \n",
       "5039                   841.0          NaN     Crime|Drama|Mystery|Thriller   \n",
       "5040                     0.0          NaN            Drama|Horror|Thriller   \n",
       "5041                   946.0      10443.0             Comedy|Drama|Romance   \n",
       "5042                    86.0      85222.0                      Documentary   \n",
       "\n",
       "      ... num_user_for_reviews language  country  content_rating       budget  \\\n",
       "0     ...               3054.0  English      USA           PG-13  237000000.0   \n",
       "1     ...               1238.0  English      USA           PG-13  300000000.0   \n",
       "2     ...                994.0  English       UK           PG-13  245000000.0   \n",
       "3     ...               2701.0  English      USA           PG-13  250000000.0   \n",
       "4     ...                  NaN      NaN      NaN             NaN          NaN   \n",
       "...   ...                  ...      ...      ...             ...          ...   \n",
       "5038  ...                  6.0  English   Canada             NaN          NaN   \n",
       "5039  ...                359.0  English      USA           TV-14          NaN   \n",
       "5040  ...                  3.0  English      USA             NaN       1400.0   \n",
       "5041  ...                  9.0  English      USA           PG-13          NaN   \n",
       "5042  ...                 84.0  English      USA              PG       1100.0   \n",
       "\n",
       "      title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
       "0         2009.0                  936.0        7.9          1.78   \n",
       "1         2007.0                 5000.0        7.1          2.35   \n",
       "2         2015.0                  393.0        6.8          2.35   \n",
       "3         2012.0                23000.0        8.5          2.35   \n",
       "4            NaN                   12.0        7.1           NaN   \n",
       "...          ...                    ...        ...           ...   \n",
       "5038      2013.0                  470.0        7.7           NaN   \n",
       "5039         NaN                  593.0        7.5         16.00   \n",
       "5040      2013.0                    0.0        6.3           NaN   \n",
       "5041      2012.0                  719.0        6.3          2.35   \n",
       "5042      2004.0                   23.0        6.6          1.85   \n",
       "\n",
       "     movie_facebook_likes  \n",
       "0                   33000  \n",
       "1                       0  \n",
       "2                   85000  \n",
       "3                  164000  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "5038                   84  \n",
       "5039                32000  \n",
       "5040                   16  \n",
       "5041                  660  \n",
       "5042                  456  \n",
       "\n",
       "[5043 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>302.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>...</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Color</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>602.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>994.0</td>\n",
       "      <td>English</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.35</td>\n",
       "      <td>85000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Color</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>813.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>164000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   color      director_name  num_critic_for_reviews  duration  \\\n",
       "0  Color      James Cameron                   723.0     178.0   \n",
       "1  Color     Gore Verbinski                   302.0     169.0   \n",
       "2  Color         Sam Mendes                   602.0     148.0   \n",
       "3  Color  Christopher Nolan                   813.0     164.0   \n",
       "\n",
       "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "1                    563.0                  1000.0     Orlando Bloom   \n",
       "2                      0.0                   161.0      Rory Kinnear   \n",
       "3                  22000.0                 23000.0    Christian Bale   \n",
       "\n",
       "   actor_1_facebook_likes        gross                           genres  ...  \\\n",
       "0                  1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi  ...   \n",
       "1                 40000.0  309404152.0         Action|Adventure|Fantasy  ...   \n",
       "2                 11000.0  200074175.0        Action|Adventure|Thriller  ...   \n",
       "3                 27000.0  448130642.0                  Action|Thriller  ...   \n",
       "\n",
       "  num_user_for_reviews language  country  content_rating       budget  \\\n",
       "0               3054.0  English      USA           PG-13  237000000.0   \n",
       "1               1238.0  English      USA           PG-13  300000000.0   \n",
       "2                994.0  English       UK           PG-13  245000000.0   \n",
       "3               2701.0  English      USA           PG-13  250000000.0   \n",
       "\n",
       "   title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
       "0      2009.0                  936.0        7.9          1.78   \n",
       "1      2007.0                 5000.0        7.1          2.35   \n",
       "2      2015.0                  393.0        6.8          2.35   \n",
       "3      2012.0                23000.0        8.5          2.35   \n",
       "\n",
       "  movie_facebook_likes  \n",
       "0                33000  \n",
       "1                    0  \n",
       "2                85000  \n",
       "3               164000  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n",
       "       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n",
       "       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n",
       "       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n",
       "       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n",
       "       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n",
       "       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n",
       "       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12215500000.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_feature(input_val, max_val = 1000000000.0):\n",
    "    return 2*(input_val-0)/(max_val-0)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_conv(col, df, non_numeric = False, use_col_max = False, use_one_mil = False, use_one_bil = False, use_five_mil = False, use_fifty_bil = False):\n",
    "    #unique_vals = df[col].unique()\n",
    "    new_vals = list(df[col])\n",
    "    \n",
    "    if non_numeric == True:\n",
    "        unique_vals = dict([x for x in enumerate(df[col].unique())])\n",
    "        unique_vals = {value: float(key) for key, value in unique_vals.items()} #this line turns keys to values and values to keys\n",
    "        new_vals = []\n",
    "\n",
    "        for val in df[col]:\n",
    "            new_vals.append(unique_vals[val])\n",
    "    \n",
    "    max_val = None\n",
    "    scaled_vals = []\n",
    "    \n",
    "    if use_col_max == True:\n",
    "        max_val = max(new_vals)\n",
    "    else:\n",
    "        max_val = 1000000000000.0\n",
    "        if use_fifty_bil == True:\n",
    "            max_val = 5000000000.0\n",
    "        if use_one_bil == True:\n",
    "            max_val = 1000000000.0\n",
    "        if use_five_mil == True:\n",
    "            max_val = 5000000.0\n",
    "        if use_one_mil == True:\n",
    "            max_val = 1000000.0\n",
    "    \n",
    "    for input_val in new_vals:\n",
    "        scaled_vals.append(scale_feature(input_val, max_val))\n",
    "    \n",
    "    df[col] = scaled_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_features(df, col):\n",
    "    import re\n",
    "    unique_genre = set()\n",
    "    for item in df[col]:\n",
    "        text = item\n",
    "        pattern = r'b|\\w+\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        unique_genre.update(matches)\n",
    "    for genre in unique_genre:\n",
    "        df[genre] = 0\n",
    "        col_content = []\n",
    "        for item in df[col]:\n",
    "            text = item\n",
    "            pattern = r'b|\\w+\\b'\n",
    "            matches = re.findall(pattern, text)\n",
    "            if genre in matches:\n",
    "                col_content.append(1)\n",
    "            else:\n",
    "                col_content.append(0)\n",
    "        df[genre] = col_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3756"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12215500000.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['budget'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.78\n",
       "1       2.35\n",
       "2       2.35\n",
       "3       2.35\n",
       "5       2.35\n",
       "        ... \n",
       "5026    2.35\n",
       "5027    1.85\n",
       "5033    1.85\n",
       "5035    1.37\n",
       "5042    1.85\n",
       "Name: aspect_ratio, Length: 3756, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['aspect_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_conv('budget', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = 0\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\2528095986.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[genre] = col_content\n"
     ]
    }
   ],
   "source": [
    "genre_features(df, 'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n",
       "       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n",
       "       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n",
       "       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n",
       "       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n",
       "       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n",
       "       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n",
       "       'imdb_score', 'aspect_ratio', 'movie_facebook_likes', 'Fantasy',\n",
       "       'Biography', 'Animation', 'Music', 'Drama', 'Film', 'Noir', 'Comedy',\n",
       "       'Documentary', 'Romance', 'Crime', 'Fi', 'Musical', 'Thriller',\n",
       "       'Mystery', 'Sport', 'Adventure', 'Family', 'War', 'Horror', 'Sci',\n",
       "       'Action', 'History', 'Western'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'content_rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_4756\\3625213529.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = scaled_vals\n"
     ]
    }
   ],
   "source": [
    "feature_conv('director_name', df, non_numeric = True, use_col_max = True)\n",
    "feature_conv('num_critic_for_reviews', df, non_numeric = False, use_col_max = True)\n",
    "feature_conv('duration', df, non_numeric = False, use_col_max = True)\n",
    "feature_conv('director_facebook_likes', df, non_numeric = False, use_col_max = True, use_one_mil=True)\n",
    "feature_conv('actor_3_facebook_likes', df, non_numeric = False, use_col_max = True, use_one_mil=True)\n",
    "feature_conv('actor_1_facebook_likes', df, non_numeric = False, use_col_max = True, use_one_mil=True)\n",
    "feature_conv('gross', df, non_numeric = False, use_col_max = False)\n",
    "feature_conv('actor_1_name', df, non_numeric = True, use_col_max = True)\n",
    "feature_conv('num_voted_users', df, non_numeric = False, use_col_max = False, use_five_mil = True)\n",
    "feature_conv('cast_total_facebook_likes', df, non_numeric = False, use_col_max = False, use_one_bil = True)\n",
    "feature_conv('num_user_for_reviews', df, non_numeric = False, use_col_max = True)\n",
    "feature_conv('language', df, non_numeric = True, use_col_max = True)\n",
    "feature_conv('country', df, non_numeric = True, use_col_max = True)\n",
    "feature_conv('content_rating', df, non_numeric = True, use_col_max = True)\n",
    "feature_conv('budget', df, non_numeric = False, use_col_max = False)\n",
    "feature_conv('imdb_score', df, non_numeric = False, use_col_max = True)\n",
    "feature_conv('aspect_ratio', df, non_numeric = False, use_col_max = True)\n",
    "feature_conv('movie_facebook_likes', df, non_numeric = False, use_col_max = False, use_one_bil = True)\n",
    "feature_conv('actor_2_facebook_likes', df, non_numeric = False, use_col_max = False, use_one_mil=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'director_name', 'num_critic_for_reviews', 'duration',\n",
    "#       'director_facebook_likes', 'actor_3_facebook_likes',\n",
    "#       'actor_1_facebook_likes', 'gross', 'actor_1_name',\n",
    " #      'num_voted_users', 'cast_total_facebook_likes',\n",
    "  #     'num_user_for_reviews', 'language', 'country',\n",
    "   #    'content_rating', 'budget', 'actor_2_facebook_likes',\n",
    "    #   'imdb_score', 'aspect_ratio', 'movie_facebook_likes',\n",
    "df = df[[ 'Fi', 'Animation',\n",
    "       'Action', 'Comedy', 'Musical', 'Western', 'Romance', 'War', 'Thriller',\n",
    "       'Documentary', 'Horror', 'Family', 'Sci', 'Sport', 'Drama', 'Fantasy',\n",
    "       'History', 'Biography', 'Adventure', 'Crime', 'Music', 'Film', 'Noir',\n",
    "       'Mystery']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cast_total_facebook_likes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fi</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Action</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Western</th>\n",
       "      <th>Romance</th>\n",
       "      <th>War</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>History</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Film</th>\n",
       "      <th>Noir</th>\n",
       "      <th>Mystery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fi</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060528</td>\n",
       "      <td>0.296424</td>\n",
       "      <td>-0.106356</td>\n",
       "      <td>-0.048223</td>\n",
       "      <td>-0.036627</td>\n",
       "      <td>-0.143120</td>\n",
       "      <td>-0.076115</td>\n",
       "      <td>0.117837</td>\n",
       "      <td>-0.042953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228037</td>\n",
       "      <td>0.013919</td>\n",
       "      <td>-0.079278</td>\n",
       "      <td>-0.101682</td>\n",
       "      <td>0.249713</td>\n",
       "      <td>-0.143944</td>\n",
       "      <td>-0.063817</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>0.026712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation</th>\n",
       "      <td>0.060528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038556</td>\n",
       "      <td>0.178670</td>\n",
       "      <td>0.166813</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.076462</td>\n",
       "      <td>-0.023887</td>\n",
       "      <td>-0.139560</td>\n",
       "      <td>-0.014835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171874</td>\n",
       "      <td>0.243640</td>\n",
       "      <td>-0.035423</td>\n",
       "      <td>-0.046454</td>\n",
       "      <td>0.301610</td>\n",
       "      <td>-0.097889</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>-0.003829</td>\n",
       "      <td>-0.003829</td>\n",
       "      <td>-0.051521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Action</th>\n",
       "      <td>0.296424</td>\n",
       "      <td>-0.038556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.191659</td>\n",
       "      <td>-0.087095</td>\n",
       "      <td>0.038968</td>\n",
       "      <td>-0.195278</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.300279</td>\n",
       "      <td>-0.058868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270289</td>\n",
       "      <td>0.043868</td>\n",
       "      <td>-0.022034</td>\n",
       "      <td>-0.117623</td>\n",
       "      <td>0.319850</td>\n",
       "      <td>0.149750</td>\n",
       "      <td>-0.094972</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>-0.048460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comedy</th>\n",
       "      <td>-0.106356</td>\n",
       "      <td>0.178670</td>\n",
       "      <td>-0.191659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050724</td>\n",
       "      <td>-0.052483</td>\n",
       "      <td>0.177968</td>\n",
       "      <td>-0.119514</td>\n",
       "      <td>-0.392448</td>\n",
       "      <td>-0.042685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259232</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>-0.145376</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>-0.077852</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.207961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musical</th>\n",
       "      <td>-0.048223</td>\n",
       "      <td>0.166813</td>\n",
       "      <td>-0.087095</td>\n",
       "      <td>0.050724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>-0.016138</td>\n",
       "      <td>-0.097985</td>\n",
       "      <td>-0.017834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>0.026895</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>-0.047946</td>\n",
       "      <td>0.087091</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.043516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western</th>\n",
       "      <td>-0.036627</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.038968</td>\n",
       "      <td>-0.052483</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>0.039251</td>\n",
       "      <td>-0.040031</td>\n",
       "      <td>-0.013911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031109</td>\n",
       "      <td>-0.037371</td>\n",
       "      <td>0.040145</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>0.040794</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>-0.014954</td>\n",
       "      <td>-0.002062</td>\n",
       "      <td>-0.002062</td>\n",
       "      <td>-0.028496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>-0.143120</td>\n",
       "      <td>-0.076462</td>\n",
       "      <td>-0.195278</td>\n",
       "      <td>0.177968</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>-0.226682</td>\n",
       "      <td>-0.059963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164911</td>\n",
       "      <td>-0.042578</td>\n",
       "      <td>-0.016487</td>\n",
       "      <td>-0.030280</td>\n",
       "      <td>-0.139981</td>\n",
       "      <td>-0.133075</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.104246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>War</th>\n",
       "      <td>-0.076115</td>\n",
       "      <td>-0.023887</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>-0.119514</td>\n",
       "      <td>-0.016138</td>\n",
       "      <td>0.039251</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050849</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165901</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>0.338982</td>\n",
       "      <td>0.101448</td>\n",
       "      <td>-0.002017</td>\n",
       "      <td>-0.081801</td>\n",
       "      <td>-0.021396</td>\n",
       "      <td>-0.003351</td>\n",
       "      <td>-0.003351</td>\n",
       "      <td>-0.042545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thriller</th>\n",
       "      <td>0.117837</td>\n",
       "      <td>-0.139560</td>\n",
       "      <td>0.300279</td>\n",
       "      <td>-0.392448</td>\n",
       "      <td>-0.097985</td>\n",
       "      <td>-0.040031</td>\n",
       "      <td>-0.226682</td>\n",
       "      <td>-0.050849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>-0.108709</td>\n",
       "      <td>-0.069563</td>\n",
       "      <td>-0.100400</td>\n",
       "      <td>-0.041997</td>\n",
       "      <td>0.349997</td>\n",
       "      <td>-0.118325</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.311067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Documentary</th>\n",
       "      <td>-0.042953</td>\n",
       "      <td>-0.014835</td>\n",
       "      <td>-0.058868</td>\n",
       "      <td>-0.042685</td>\n",
       "      <td>-0.017834</td>\n",
       "      <td>-0.013911</td>\n",
       "      <td>-0.059963</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>-0.066289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071847</td>\n",
       "      <td>-0.043500</td>\n",
       "      <td>0.052843</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>-0.050391</td>\n",
       "      <td>-0.021852</td>\n",
       "      <td>0.089581</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.037161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horror</th>\n",
       "      <td>0.093202</td>\n",
       "      <td>-0.076182</td>\n",
       "      <td>-0.068076</td>\n",
       "      <td>-0.179479</td>\n",
       "      <td>-0.038731</td>\n",
       "      <td>-0.043124</td>\n",
       "      <td>-0.163075</td>\n",
       "      <td>-0.065685</td>\n",
       "      <td>0.183691</td>\n",
       "      <td>-0.037590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225664</td>\n",
       "      <td>0.094512</td>\n",
       "      <td>-0.064919</td>\n",
       "      <td>-0.085420</td>\n",
       "      <td>-0.108384</td>\n",
       "      <td>-0.122384</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.005571</td>\n",
       "      <td>-0.005571</td>\n",
       "      <td>0.177992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family</th>\n",
       "      <td>0.028387</td>\n",
       "      <td>0.568188</td>\n",
       "      <td>-0.073623</td>\n",
       "      <td>0.232311</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.057220</td>\n",
       "      <td>-0.066615</td>\n",
       "      <td>-0.219522</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202876</td>\n",
       "      <td>0.339337</td>\n",
       "      <td>-0.065759</td>\n",
       "      <td>-0.074893</td>\n",
       "      <td>0.338159</td>\n",
       "      <td>-0.146606</td>\n",
       "      <td>0.017795</td>\n",
       "      <td>-0.005960</td>\n",
       "      <td>-0.005960</td>\n",
       "      <td>-0.065969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sci</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060528</td>\n",
       "      <td>0.296424</td>\n",
       "      <td>-0.106356</td>\n",
       "      <td>-0.048223</td>\n",
       "      <td>-0.036627</td>\n",
       "      <td>-0.143120</td>\n",
       "      <td>-0.076115</td>\n",
       "      <td>0.117837</td>\n",
       "      <td>-0.042953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228037</td>\n",
       "      <td>0.013919</td>\n",
       "      <td>-0.079278</td>\n",
       "      <td>-0.101682</td>\n",
       "      <td>0.249713</td>\n",
       "      <td>-0.143944</td>\n",
       "      <td>-0.063817</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>0.026712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>-0.062832</td>\n",
       "      <td>-0.016756</td>\n",
       "      <td>-0.058964</td>\n",
       "      <td>0.006825</td>\n",
       "      <td>-0.032801</td>\n",
       "      <td>-0.025586</td>\n",
       "      <td>-0.022312</td>\n",
       "      <td>-0.027704</td>\n",
       "      <td>-0.119791</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>-0.063988</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.080212</td>\n",
       "      <td>-0.041451</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>-0.068347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drama</th>\n",
       "      <td>-0.228037</td>\n",
       "      <td>-0.171874</td>\n",
       "      <td>-0.270289</td>\n",
       "      <td>-0.259232</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.031109</td>\n",
       "      <td>0.164911</td>\n",
       "      <td>0.165901</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>-0.071847</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.208078</td>\n",
       "      <td>0.174346</td>\n",
       "      <td>0.234612</td>\n",
       "      <td>-0.276358</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>-0.009726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fantasy</th>\n",
       "      <td>0.013919</td>\n",
       "      <td>0.243640</td>\n",
       "      <td>0.043868</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>-0.037371</td>\n",
       "      <td>-0.042578</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>-0.108709</td>\n",
       "      <td>-0.043500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.080288</td>\n",
       "      <td>-0.099785</td>\n",
       "      <td>0.264134</td>\n",
       "      <td>-0.162678</td>\n",
       "      <td>-0.033249</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>-0.030434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>-0.079278</td>\n",
       "      <td>-0.035423</td>\n",
       "      <td>-0.022034</td>\n",
       "      <td>-0.145376</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>0.040145</td>\n",
       "      <td>-0.016487</td>\n",
       "      <td>0.338982</td>\n",
       "      <td>-0.069563</td>\n",
       "      <td>0.052843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174346</td>\n",
       "      <td>-0.080288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332605</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>-0.059697</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.064085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biography</th>\n",
       "      <td>-0.101682</td>\n",
       "      <td>-0.046454</td>\n",
       "      <td>-0.117623</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.026895</td>\n",
       "      <td>0.010927</td>\n",
       "      <td>-0.030280</td>\n",
       "      <td>0.101448</td>\n",
       "      <td>-0.100400</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234612</td>\n",
       "      <td>-0.099785</td>\n",
       "      <td>0.332605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.079813</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>0.063254</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>-0.084370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adventure</th>\n",
       "      <td>0.249713</td>\n",
       "      <td>0.301610</td>\n",
       "      <td>0.319850</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>0.040794</td>\n",
       "      <td>-0.139981</td>\n",
       "      <td>-0.002017</td>\n",
       "      <td>-0.041997</td>\n",
       "      <td>-0.050391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276358</td>\n",
       "      <td>0.264134</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>-0.079813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.180096</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>-0.077624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>-0.143944</td>\n",
       "      <td>-0.097889</td>\n",
       "      <td>0.149750</td>\n",
       "      <td>-0.077852</td>\n",
       "      <td>-0.047946</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>-0.133075</td>\n",
       "      <td>-0.081801</td>\n",
       "      <td>0.349997</td>\n",
       "      <td>-0.021852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>-0.162678</td>\n",
       "      <td>-0.059697</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>-0.180096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053697</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.099967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>-0.063817</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>-0.094972</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>0.087091</td>\n",
       "      <td>-0.014954</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>-0.021396</td>\n",
       "      <td>-0.118325</td>\n",
       "      <td>0.089581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056648</td>\n",
       "      <td>-0.033249</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.063254</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>-0.053697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>-0.055644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.003829</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.002062</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.003351</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noir</th>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.003829</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.002062</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.003351</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>-0.006447</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mystery</th>\n",
       "      <td>0.026712</td>\n",
       "      <td>-0.051521</td>\n",
       "      <td>-0.048460</td>\n",
       "      <td>-0.207961</td>\n",
       "      <td>-0.043516</td>\n",
       "      <td>-0.028496</td>\n",
       "      <td>-0.104246</td>\n",
       "      <td>-0.042545</td>\n",
       "      <td>0.311067</td>\n",
       "      <td>-0.037161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009726</td>\n",
       "      <td>-0.030434</td>\n",
       "      <td>-0.064085</td>\n",
       "      <td>-0.084370</td>\n",
       "      <td>-0.077624</td>\n",
       "      <td>0.099967</td>\n",
       "      <td>-0.055644</td>\n",
       "      <td>0.048359</td>\n",
       "      <td>0.048359</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Fi  Animation    Action    Comedy   Musical   Western  \\\n",
       "Fi           1.000000   0.060528  0.296424 -0.106356 -0.048223 -0.036627   \n",
       "Animation    0.060528   1.000000 -0.038556  0.178670  0.166813 -0.000759   \n",
       "Action       0.296424  -0.038556  1.000000 -0.191659 -0.087095  0.038968   \n",
       "Comedy      -0.106356   0.178670 -0.191659  1.000000  0.050724 -0.052483   \n",
       "Musical     -0.048223   0.166813 -0.087095  0.050724  1.000000 -0.006892   \n",
       "Western     -0.036627  -0.000759  0.038968 -0.052483 -0.006892  1.000000   \n",
       "Romance     -0.143120  -0.076462 -0.195278  0.177968  0.068465 -0.002515   \n",
       "War         -0.076115  -0.023887  0.028477 -0.119514 -0.016138  0.039251   \n",
       "Thriller     0.117837  -0.139560  0.300279 -0.392448 -0.097985 -0.040031   \n",
       "Documentary -0.042953  -0.014835 -0.058868 -0.042685 -0.017834 -0.013911   \n",
       "Horror       0.093202  -0.076182 -0.068076 -0.179479 -0.038731 -0.043124   \n",
       "Family       0.028387   0.568188 -0.073623  0.232311  0.176453 -0.026201   \n",
       "Sci          1.000000   0.060528  0.296424 -0.106356 -0.048223 -0.036627   \n",
       "Sport       -0.062832  -0.016756 -0.058964  0.006825 -0.032801 -0.025586   \n",
       "Drama       -0.228037  -0.171874 -0.270289 -0.259232  0.002081  0.031109   \n",
       "Fantasy      0.013919   0.243640  0.043868  0.036420  0.069324 -0.037371   \n",
       "History     -0.079278  -0.035423 -0.022034 -0.145376 -0.006986  0.040145   \n",
       "Biography   -0.101682  -0.046454 -0.117623 -0.154297  0.026895  0.010927   \n",
       "Adventure    0.249713   0.301610  0.319850 -0.033361  0.020944  0.040794   \n",
       "Crime       -0.143944  -0.097889  0.149750 -0.077852 -0.047946  0.004721   \n",
       "Music       -0.063817  -0.005361 -0.094972  0.028536  0.087091 -0.014954   \n",
       "Film        -0.006365  -0.003829 -0.009556 -0.013021 -0.002643 -0.002062   \n",
       "Noir        -0.006365  -0.003829 -0.009556 -0.013021 -0.002643 -0.002062   \n",
       "Mystery      0.026712  -0.051521 -0.048460 -0.207961 -0.043516 -0.028496   \n",
       "\n",
       "              Romance       War  Thriller  Documentary  ...     Drama  \\\n",
       "Fi          -0.143120 -0.076115  0.117837    -0.042953  ... -0.228037   \n",
       "Animation   -0.076462 -0.023887 -0.139560    -0.014835  ... -0.171874   \n",
       "Action      -0.195278  0.028477  0.300279    -0.058868  ... -0.270289   \n",
       "Comedy       0.177968 -0.119514 -0.392448    -0.042685  ... -0.259232   \n",
       "Musical      0.068465 -0.016138 -0.097985    -0.017834  ...  0.002081   \n",
       "Western     -0.002515  0.039251 -0.040031    -0.013911  ...  0.031109   \n",
       "Romance      1.000000 -0.002453 -0.226682    -0.059963  ...  0.164911   \n",
       "War         -0.002453  1.000000 -0.050849     0.027058  ...  0.165901   \n",
       "Thriller    -0.226682 -0.050849  1.000000    -0.066289  ... -0.023253   \n",
       "Documentary -0.059963  0.027058 -0.066289     1.000000  ... -0.071847   \n",
       "Horror      -0.163075 -0.065685  0.183691    -0.037590  ... -0.225664   \n",
       "Family      -0.057220 -0.066615 -0.219522    -0.040216  ... -0.202876   \n",
       "Sci         -0.143120 -0.076115  0.117837    -0.042953  ... -0.228037   \n",
       "Sport       -0.022312 -0.027704 -0.119791     0.028009  ...  0.077756   \n",
       "Drama        0.164911  0.165901 -0.023253    -0.071847  ...  1.000000   \n",
       "Fantasy     -0.042578 -0.057402 -0.108709    -0.043500  ... -0.208078   \n",
       "History     -0.016487  0.338982 -0.069563     0.052843  ...  0.174346   \n",
       "Biography   -0.030280  0.101448 -0.100400     0.051495  ...  0.234612   \n",
       "Adventure   -0.139981 -0.002017 -0.041997    -0.050391  ... -0.276358   \n",
       "Crime       -0.133075 -0.081801  0.349997    -0.021852  ...  0.051258   \n",
       "Music        0.053138 -0.021396 -0.118325     0.089581  ...  0.056648   \n",
       "Film        -0.008886 -0.003351  0.025084    -0.001797  ...  0.016189   \n",
       "Noir        -0.008886 -0.003351  0.025084    -0.001797  ...  0.016189   \n",
       "Mystery     -0.104246 -0.042545  0.311067    -0.037161  ... -0.009726   \n",
       "\n",
       "              Fantasy   History  Biography  Adventure     Crime     Music  \\\n",
       "Fi           0.013919 -0.079278  -0.101682   0.249713 -0.143944 -0.063817   \n",
       "Animation    0.243640 -0.035423  -0.046454   0.301610 -0.097889 -0.005361   \n",
       "Action       0.043868 -0.022034  -0.117623   0.319850  0.149750 -0.094972   \n",
       "Comedy       0.036420 -0.145376  -0.154297  -0.033361 -0.077852  0.028536   \n",
       "Musical      0.069324 -0.006986   0.026895   0.020944 -0.047946  0.087091   \n",
       "Western     -0.037371  0.040145   0.010927   0.040794  0.004721 -0.014954   \n",
       "Romance     -0.042578 -0.016487  -0.030280  -0.139981 -0.133075  0.053138   \n",
       "War         -0.057402  0.338982   0.101448  -0.002017 -0.081801 -0.021396   \n",
       "Thriller    -0.108709 -0.069563  -0.100400  -0.041997  0.349997 -0.118325   \n",
       "Documentary -0.043500  0.052843   0.051495  -0.050391 -0.021852  0.089581   \n",
       "Horror       0.094512 -0.064919  -0.085420  -0.108384 -0.122384 -0.065430   \n",
       "Family       0.339337 -0.065759  -0.074893   0.338159 -0.146606  0.017795   \n",
       "Sci          0.013919 -0.079278  -0.101682   0.249713 -0.143944 -0.063817   \n",
       "Sport       -0.063988  0.000903   0.160242  -0.073424 -0.080212 -0.041451   \n",
       "Drama       -0.208078  0.174346   0.234612  -0.276358  0.051258  0.056648   \n",
       "Fantasy      1.000000 -0.080288  -0.099785   0.264134 -0.162678 -0.033249   \n",
       "History     -0.080288  1.000000   0.332605  -0.003301 -0.059697  0.000068   \n",
       "Biography   -0.099785  0.332605   1.000000  -0.079813 -0.011469  0.063254   \n",
       "Adventure    0.264134 -0.003301  -0.079813   1.000000 -0.180096 -0.064785   \n",
       "Crime       -0.162678 -0.059697  -0.011469  -0.180096  1.000000 -0.053697   \n",
       "Music       -0.033249  0.000068   0.063254  -0.064785 -0.053697  1.000000   \n",
       "Film        -0.006447 -0.003317  -0.004254  -0.008361  0.033830 -0.003340   \n",
       "Noir        -0.006447 -0.003317  -0.004254  -0.008361  0.033830 -0.003340   \n",
       "Mystery     -0.030434 -0.064085  -0.084370  -0.077624  0.099967 -0.055644   \n",
       "\n",
       "                 Film      Noir   Mystery  \n",
       "Fi          -0.006365 -0.006365  0.026712  \n",
       "Animation   -0.003829 -0.003829 -0.051521  \n",
       "Action      -0.009556 -0.009556 -0.048460  \n",
       "Comedy      -0.013021 -0.013021 -0.207961  \n",
       "Musical     -0.002643 -0.002643 -0.043516  \n",
       "Western     -0.002062 -0.002062 -0.028496  \n",
       "Romance     -0.008886 -0.008886 -0.104246  \n",
       "War         -0.003351 -0.003351 -0.042545  \n",
       "Thriller     0.025084  0.025084  0.311067  \n",
       "Documentary -0.001797 -0.001797 -0.037161  \n",
       "Horror      -0.005571 -0.005571  0.177992  \n",
       "Family      -0.005960 -0.005960 -0.065969  \n",
       "Sci         -0.006365 -0.006365  0.026712  \n",
       "Sport       -0.003305 -0.003305 -0.068347  \n",
       "Drama        0.016189  0.016189 -0.009726  \n",
       "Fantasy     -0.006447 -0.006447 -0.030434  \n",
       "History     -0.003317 -0.003317 -0.064085  \n",
       "Biography   -0.004254 -0.004254 -0.084370  \n",
       "Adventure   -0.008361 -0.008361 -0.077624  \n",
       "Crime        0.033830  0.033830  0.099967  \n",
       "Music       -0.003340 -0.003340 -0.055644  \n",
       "Film         1.000000  1.000000  0.048359  \n",
       "Noir         1.000000  1.000000  0.048359  \n",
       "Mystery      0.048359  0.048359  1.000000  \n",
       "\n",
       "[24 rows x 24 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fi</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Action</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Western</th>\n",
       "      <th>Romance</th>\n",
       "      <th>War</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>History</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Film</th>\n",
       "      <th>Noir</th>\n",
       "      <th>Mystery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3756 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fi  Animation  Action  Comedy  Musical  Western  Romance  War  Thriller  \\\n",
       "0      1          0       1       0        0        0        0    0         0   \n",
       "1      0          0       1       0        0        0        0    0         0   \n",
       "2      0          0       1       0        0        0        0    0         1   \n",
       "3      0          0       1       0        0        0        0    0         1   \n",
       "5      1          0       1       0        0        0        0    0         0   \n",
       "...   ..        ...     ...     ...      ...      ...      ...  ...       ...   \n",
       "5026   0          0       0       0        0        0        1    0         0   \n",
       "5027   0          0       0       0        0        0        0    0         0   \n",
       "5033   1          0       0       0        0        0        0    0         1   \n",
       "5035   0          0       1       0        0        0        1    0         1   \n",
       "5042   0          0       0       0        0        0        0    0         0   \n",
       "\n",
       "      Documentary  ...  Drama  Fantasy  History  Biography  Adventure  Crime  \\\n",
       "0               0  ...      0        1        0          0          1      0   \n",
       "1               0  ...      0        1        0          0          1      0   \n",
       "2               0  ...      0        0        0          0          1      0   \n",
       "3               0  ...      0        0        0          0          0      0   \n",
       "5               0  ...      0        0        0          0          1      0   \n",
       "...           ...  ...    ...      ...      ...        ...        ...    ...   \n",
       "5026            0  ...      1        0        0          0          0      0   \n",
       "5027            0  ...      1        0        0          0          0      0   \n",
       "5033            0  ...      1        0        0          0          0      0   \n",
       "5035            0  ...      1        0        0          0          0      1   \n",
       "5042            1  ...      0        0        0          0          0      0   \n",
       "\n",
       "      Music  Film  Noir  Mystery  \n",
       "0         0     0     0        0  \n",
       "1         0     0     0        0  \n",
       "2         0     0     0        0  \n",
       "3         0     0     0        0  \n",
       "5         0     0     0        0  \n",
       "...     ...   ...   ...      ...  \n",
       "5026      1     0     0        0  \n",
       "5027      0     0     0        0  \n",
       "5033      0     0     0        0  \n",
       "5035      0     0     0        0  \n",
       "5042      0     0     0        0  \n",
       "\n",
       "[3756 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#create a column and call it my likes, that'll be the target col/.'?'\n",
    "#likes = [random.randint(0, 1) for value in df['imdb_score']]\n",
    "likes = [1 if value >= 1 else 0 for value in df['Adventure']]\n",
    "df['likes'] = likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2975\n",
       "1     781\n",
       "Name: likes, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['likes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fi</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Action</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Western</th>\n",
       "      <th>Romance</th>\n",
       "      <th>War</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>History</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Film</th>\n",
       "      <th>Noir</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fi  Animation  Action  Comedy  Musical  Western  Romance  War  Thriller  \\\n",
       "0   1          0       1       0        0        0        0    0         0   \n",
       "1   0          0       1       0        0        0        0    0         0   \n",
       "2   0          0       1       0        0        0        0    0         1   \n",
       "\n",
       "   Documentary  ...  Fantasy  History  Biography  Adventure  Crime  Music  \\\n",
       "0            0  ...        1        0          0          1      0      0   \n",
       "1            0  ...        1        0          0          1      0      0   \n",
       "2            0  ...        0        0          0          1      0      0   \n",
       "\n",
       "   Film  Noir  Mystery  likes  \n",
       "0     0     0        0      1  \n",
       "1     0     0        0      1  \n",
       "2     0     0        0      1  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#create an ANN class\n",
    "class movieModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(movieModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size*4)\n",
    "        self.fc2 = nn.Linear(input_size*4, input_size*3)\n",
    "        self.fc3 = nn.Linear(input_size*3, input_size*2)\n",
    "        self.fc4 = nn.Linear(input_size*2, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.inp_s = input_size\n",
    "    def forward(self, x):\n",
    "        #x = x.double()\n",
    "        #x = x.view(43, 1)\n",
    "        #print(self.inp_s)\n",
    "        #print(x)\n",
    "        #print(self.fc1.weight.dtype)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.sigmoid(self.fc5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, i):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3756"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[int((20/100)*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data load it and split train and test\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "len_df = int((75/100)*len(df))\n",
    "train_df = df[:len_df]\n",
    "test_df = df[len_df:]\n",
    "\n",
    "# Tensor transform\n",
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# training datasets\n",
    "#svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
    "train_df = transforms(train_df.values)[0]\n",
    "test_df = transforms(test_df.values)[0]\n",
    "\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "\n",
    "print(train_df)\n",
    "#batched_data = torch.stack([train_df[i:i+batch_size] for i in range(0, len(train_df), batch_size)])\n",
    "def batch_maker():\n",
    "    b = []\n",
    "    for i in range(len(train_df)-batch_size):\n",
    "        b.append((train_df[i:i+batch_size]))\n",
    "    return b\n",
    "#print(batched_data)\n",
    "\n",
    "# build DataLoaders for SVHN dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_df,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_df,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)\n",
    "train_loader = torch.stack(batch_maker())\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "len_df = int((75/100)*len(df))\n",
    "train_df = df[:len_df]\n",
    "test_df = df[len_df:]\n",
    "\n",
    "def perfect_batches(df, batch_size):\n",
    "    batched_data = []\n",
    "    if len(df)%batch_size != 0:\n",
    "        print('Cannot create a perfect batch size with the input. \\nTry the following: ')\n",
    "        for i in range(len(df)):\n",
    "            if i != 0 and len(df)%i == 0:\n",
    "                print('This number can do: ', i)\n",
    "        print('\\n\\nEnd of numbers!')\n",
    "        return None\n",
    "    for i in range(len(df)-batch_size+1):\n",
    "        batch = []\n",
    "        i_inc = i\n",
    "        for j in range(batch_size):\n",
    "            #print(df.values[i])\n",
    "            batch.append(torch.tensor(df.values[i_inc]))\n",
    "            i_inc += 1\n",
    "        #print(batch)\n",
    "        batched_data.append(torch.stack(batch))\n",
    "    return torch.stack(batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_batches2(df, batch_size):\n",
    "    batched_data = []\n",
    "    if len(df)%batch_size != 0:\n",
    "        print('Cannot create a perfect batch size with the input. \\nTry the following: ')\n",
    "        for i in range(len(df)):\n",
    "            if i != 0 and len(df)%i == 0:\n",
    "                print('This number can do: ', i)\n",
    "        print('\\n\\nEnd of numbers!')\n",
    "        return None\n",
    "    i = 0\n",
    "    while i <= len(df)-batch_size+1:\n",
    "        batch = []\n",
    "        j = 0\n",
    "        while j <= batch_size:\n",
    "            batch.append(torch.tensor(df.values[i]))\n",
    "            i += 1\n",
    "            j += 1\n",
    "        batched_data.append(torch.stack(batch))\n",
    "    return torch.stack(batched_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train = perfect_batches2(train_df, 313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_test = perfect_batches2(test_df, 313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.stack(batch_maker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2817"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelAndOptim(input_len, lr):\n",
    "    #modell = movieModel(input_len)\n",
    "    modell = BinaryClassifier(input_len)\n",
    "    optimizer = optim.SGD(modell.parameters(), lr)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    return modell, optimizer, loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#v = torch.tensor([1.0, 6.9, 0, 8.0, 1.1, 0, 7.0, 2.4, 9.0, 8,4])\n",
    "#a = torch.tensor([1.0, 9.9, 0, 6.0, 1.1, 0, 2.0, 2.4, 9.0, 8,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, actual):\n",
    "    pred = torch.tensor([1.0 if i >= 0.7 else 0.0 for i in pred])\n",
    "    #print('preddd: ', pred)\n",
    "    #actual = torch.tensor([1 if i >= 7.0 else 0 for i in actual])\n",
    "    same = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == actual[i]:\n",
    "            same.append(1)\n",
    "        else:\n",
    "            same.append(0)\n",
    "    same = torch.tensor(same)\n",
    "    #print(pred)\n",
    "    #print(actual)\n",
    "    #print(same)\n",
    "    return (sum(same)/len(same)).item()\n",
    "    \n",
    "#accuracy(v, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another train model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a train method and validation\n",
    "#Theres an error i spotted i have to make sure that it loops through the batch one at a time not all at once\n",
    "model, optimizer, loss = modelAndOptim(24, 0.008)\n",
    "def train(epoch, train_data, model, optimizer, loss):\n",
    "    model.train()\n",
    "    #model = model.double()\n",
    "    sum_loss = []\n",
    "    accuracy_count = []\n",
    "    for e in range(epoch):\n",
    "        losses = []\n",
    "        #acc = []\n",
    "        acc = None\n",
    "        \n",
    "        #train_data = next(iter(train_data)).type(torch.float32)\n",
    "        #print('train_data')\n",
    "        #print(train_data)\n",
    "        #print(train_data[0, :-1])\n",
    "        #print(train_data[0, :])\n",
    "        for batch in train_data:\n",
    "            #for rec in batch:\n",
    "            features = batch[:, :-1]\n",
    "            #print(features)\n",
    "            label = batch[:, -1]\n",
    "            #print(features.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted = model(features)\n",
    "            #print('predited')\n",
    "            #print('p: ', predicted)\n",
    "            #print('l: ', label)\n",
    "            predicted = predicted.view(314, 1)\n",
    "            label = label.view(314, 1)\n",
    "            #print('pridicted size: ', predicted.shape)\n",
    "            #print('label dType: ', label.shape)\n",
    "            #print('pridicted: ', predicted)\n",
    "            #print('label: ', label)\n",
    "            loss_error = loss(predicted.double(), label.double())\n",
    "            losses.append(loss_error.item())\n",
    "            loss_error.backward()\n",
    "            optimizer.step()\n",
    "            #acc.append(accuracy(predicted.view(-1), label.view(-1)))\n",
    "            acc = accuracy(predicted.view(-1), label.view(-1))\n",
    "            #print('Average error at current  ', sum(losses)/len(losses))\n",
    "            #print('done')\n",
    "        #print(predicted)\n",
    "        print('Average error at ',e,' epoch:  ', sum(losses)/len(losses))\n",
    "        #acr = sum(acc)/len(acc)\n",
    "        acr = acc\n",
    "        print('Accuracy: ', acr)\n",
    "        accuracy_count.append(acr)\n",
    "        sum_loss.append(sum(losses)/len(losses))\n",
    "        if acr >= 0.999:\n",
    "            plt.plot(sum_loss, label = 'loss')\n",
    "            #print(sum_loss)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            return predicted, label\n",
    "    print('\\nOverall Accuracy: ', sum(accuracy_count)/len(accuracy_count))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    #plt.plot(label, label = 'True')\n",
    "    plt.plot(sum_loss, label = 'loss')\n",
    "    #print(sum_loss)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('end of train')\n",
    "    print(\"\")\n",
    "    return predicted, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            print(data)\n",
    "            inputs, labels = data[:, :-1], data[:, -1]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print('Epoch [%d/%d], Loss: %.4f' %\n",
    "              (epoch+1, num_epochs, running_loss / len(train_loader)))\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_data, model, optimizer, loss):\n",
    "    model.eval()\n",
    "    #model = model.float()\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    for e in range(epoch):\n",
    "        losses = []\n",
    "        print(len(test_data))\n",
    "        \n",
    "        #train_data = next(iter(train_data)).type(torch.float32)\n",
    "        #print('train_data')\n",
    "        #print(train_data)\n",
    "        #print(train_data[0, :-1])\n",
    "        #print(train_data[0, :])\n",
    "        for batch in test_data:\n",
    "            features = batch[:, :-1]\n",
    "            #print(features)\n",
    "            label = batch[:, -1]\n",
    "            #print(features.shape)\n",
    "\n",
    "            predicted = model(features)\n",
    "            #print(predicted)\n",
    "            predicted = predicted.view(314, 1)\n",
    "            pred_labels.append(predicted)\n",
    "            label = label.view(314, 1)\n",
    "            actual_labels.append(label)\n",
    "            #print('pridicted size: ', predicted.shape)\n",
    "            #print('label dType: ', label.shape)\n",
    "            #print('pridicted: ', predicted)\n",
    "            #print('label: ', label)\n",
    "            loss_error = loss(predicted, label)\n",
    "            losses.append(loss_error.item())\n",
    "            acc = accuracy(predicted.view(-1), label.view(-1))\n",
    "            print('Accuracy: ', acc)\n",
    "            #loss_error.backward()\n",
    "            #optimizer.step()\n",
    "            #print('done')\n",
    "        #print(predicted)\n",
    "        print('Average error at ',e,' epoch:  ', sum(losses)/len(losses))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print('end of test')\n",
    "    print(\"\")\n",
    "    return pred_labels, actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, batched_train, optimizer, loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  0  epoch:   0.8393479955215407\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  1  epoch:   0.835035245874156\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  2  epoch:   0.8311182629105343\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  3  epoch:   0.8286827114837039\n",
      "Accuracy:  0.7802547812461853\n",
      "Average error at  4  epoch:   0.8232515108132787\n",
      "Accuracy:  0.7802547812461853\n",
      "Average error at  5  epoch:   0.820809929900526\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  6  epoch:   0.817780183005872\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  7  epoch:   0.8140049329150987\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  8  epoch:   0.8112053565194646\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  9  epoch:   0.8085221383891569\n",
      "Accuracy:  0.7802547812461853\n",
      "Average error at  10  epoch:   0.8045970235739437\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  11  epoch:   0.8019610367161055\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  12  epoch:   0.7997108179447416\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  13  epoch:   0.7973646376643677\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  14  epoch:   0.7945137441821026\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  15  epoch:   0.7914908435350301\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  16  epoch:   0.7899334041043352\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  17  epoch:   0.7880692168741285\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  18  epoch:   0.7838482752295037\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  19  epoch:   0.7821520400444687\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  20  epoch:   0.7802028814750712\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  21  epoch:   0.7792890293321852\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  22  epoch:   0.7758950258557487\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  23  epoch:   0.7726992777883096\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  24  epoch:   0.7705405689726721\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  25  epoch:   0.7689015740370283\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  26  epoch:   0.7672709423187956\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  27  epoch:   0.766472818975096\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  28  epoch:   0.7637304444300393\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  29  epoch:   0.7618303619703899\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  30  epoch:   0.7611345417813052\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  31  epoch:   0.7588975988176233\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  32  epoch:   0.7570940529456575\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  33  epoch:   0.7555646646963872\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  34  epoch:   0.7530770853463675\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  35  epoch:   0.75247885614712\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  36  epoch:   0.7499301548233454\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  37  epoch:   0.7494701816231668\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  38  epoch:   0.7470199103553026\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  39  epoch:   0.7450411056619158\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  40  epoch:   0.7447570137632818\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  41  epoch:   0.7436237294411548\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  42  epoch:   0.7421567368623447\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  43  epoch:   0.7413293110141344\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  44  epoch:   0.7410133233795202\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  45  epoch:   0.739900205540738\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  46  epoch:   0.7372472616905024\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  47  epoch:   0.7360384986311289\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  48  epoch:   0.7337105068377681\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  49  epoch:   0.733648772693411\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  50  epoch:   0.7336061200270344\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  51  epoch:   0.7319278409161383\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  52  epoch:   0.7311667820254558\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  53  epoch:   0.7306526116899579\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  54  epoch:   0.7289072176833152\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  55  epoch:   0.72815038687509\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  56  epoch:   0.7271888318672249\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  57  epoch:   0.726547304797446\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  58  epoch:   0.7249351549314529\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  59  epoch:   0.7229385901264862\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  60  epoch:   0.722855553741738\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  61  epoch:   0.7225450310245295\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  62  epoch:   0.7216811487938948\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  63  epoch:   0.7214262811640941\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  64  epoch:   0.7190265499441669\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  65  epoch:   0.7190743550563804\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  66  epoch:   0.717097230543205\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  67  epoch:   0.7172266474276955\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  68  epoch:   0.7170842039438531\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  69  epoch:   0.7145940909137116\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  70  epoch:   0.7136708305885524\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  71  epoch:   0.7137215461003589\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  72  epoch:   0.7144598238021932\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  73  epoch:   0.7125282861824268\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  74  epoch:   0.7117513359786098\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  75  epoch:   0.7096989949686636\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  76  epoch:   0.7108693333620036\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  77  epoch:   0.7108358282825806\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  78  epoch:   0.7090230140100691\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  79  epoch:   0.7076721045077851\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  80  epoch:   0.7081826000494513\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  81  epoch:   0.7063275125039306\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  82  epoch:   0.7060092668412877\n",
      "Accuracy:  0.7993630766868591\n",
      "Average error at  83  epoch:   0.7065880330149112\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  84  epoch:   0.7044384383594983\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  85  epoch:   0.704096819031695\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  86  epoch:   0.7031714871463497\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  87  epoch:   0.7039387369261879\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  88  epoch:   0.7026966546759618\n",
      "Accuracy:  0.7866241931915283\n",
      "Average error at  89  epoch:   0.7017855536694317\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  90  epoch:   0.7010717786109695\n",
      "Accuracy:  0.7834395170211792\n",
      "Average error at  91  epoch:   0.7003596902899261\n",
      "Accuracy:  0.7961783409118652\n",
      "Average error at  92  epoch:   0.6993723065446358\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  93  epoch:   0.6991362963544232\n",
      "Accuracy:  0.7961783409118652\n",
      "Average error at  94  epoch:   0.6981437513357768\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  95  epoch:   0.6976946246408374\n",
      "Accuracy:  0.7961783409118652\n",
      "Average error at  96  epoch:   0.6971222851720009\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  97  epoch:   0.6973243293026999\n",
      "Accuracy:  0.7961783409118652\n",
      "Average error at  98  epoch:   0.696376247494077\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  99  epoch:   0.6954500650743364\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  100  epoch:   0.695371041179401\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  101  epoch:   0.6945631767707605\n",
      "Accuracy:  0.7993630766868591\n",
      "Average error at  102  epoch:   0.6929630782728999\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  103  epoch:   0.693965489130868\n",
      "Accuracy:  0.7898089289665222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  104  epoch:   0.6923844513005841\n",
      "Accuracy:  0.8025477528572083\n",
      "Average error at  105  epoch:   0.6925722782363096\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  106  epoch:   0.6916756986908962\n",
      "Accuracy:  0.7961783409118652\n",
      "Average error at  107  epoch:   0.6919432328453804\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  108  epoch:   0.6904452271529403\n",
      "Accuracy:  0.7993630766868591\n",
      "Average error at  109  epoch:   0.6905010902933953\n",
      "Accuracy:  0.808917224407196\n",
      "Average error at  110  epoch:   0.6905150628185964\n",
      "Accuracy:  0.8025477528572083\n",
      "Average error at  111  epoch:   0.6904895894192304\n",
      "Accuracy:  0.7993630766868591\n",
      "Average error at  112  epoch:   0.6896899342192006\n",
      "Accuracy:  0.7898089289665222\n",
      "Average error at  113  epoch:   0.6880167379106212\n",
      "Accuracy:  0.808917224407196\n",
      "Average error at  114  epoch:   0.6893367290544937\n",
      "Accuracy:  0.7929936051368713\n",
      "Average error at  115  epoch:   0.6885217607892564\n",
      "Accuracy:  0.8025477528572083\n",
      "Average error at  116  epoch:   0.6866116003831474\n",
      "Accuracy:  0.8121019005775452\n",
      "Average error at  117  epoch:   0.68730379755744\n",
      "Accuracy:  0.8121019005775452\n",
      "Average error at  118  epoch:   0.686316327990495\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  119  epoch:   0.6854351144388603\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  120  epoch:   0.6850866543445148\n",
      "Accuracy:  0.8057324886322021\n",
      "Average error at  121  epoch:   0.6844023002712448\n",
      "Accuracy:  0.8025477528572083\n",
      "Average error at  122  epoch:   0.6841877196725596\n",
      "Accuracy:  0.8025477528572083\n",
      "Average error at  123  epoch:   0.6836071518703533\n",
      "Accuracy:  0.8121019005775452\n",
      "Average error at  124  epoch:   0.6824785937291067\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  125  epoch:   0.6827109224904622\n",
      "Accuracy:  0.808917224407196\n",
      "Average error at  126  epoch:   0.6826302759258609\n",
      "Accuracy:  0.8216560482978821\n",
      "Average error at  127  epoch:   0.6813830689122357\n",
      "Accuracy:  0.824840784072876\n",
      "Average error at  128  epoch:   0.6812104772319969\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  129  epoch:   0.6811508241520994\n",
      "Accuracy:  0.8184713125228882\n",
      "Average error at  130  epoch:   0.6792527953458073\n",
      "Accuracy:  0.8057324886322021\n",
      "Average error at  131  epoch:   0.6799895878337994\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  132  epoch:   0.6797633927531865\n",
      "Accuracy:  0.8280254602432251\n",
      "Average error at  133  epoch:   0.6789538226575821\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  134  epoch:   0.6784976052913871\n",
      "Accuracy:  0.8184713125228882\n",
      "Average error at  135  epoch:   0.6783661146449849\n",
      "Accuracy:  0.8152866363525391\n",
      "Average error at  136  epoch:   0.6784849512066845\n",
      "Accuracy:  0.824840784072876\n",
      "Average error at  137  epoch:   0.6773907120900212\n",
      "Accuracy:  0.831210196018219\n",
      "Average error at  138  epoch:   0.6765968532927381\n",
      "Accuracy:  0.8280254602432251\n",
      "Average error at  139  epoch:   0.6768187009450378\n",
      "Accuracy:  0.8184713125228882\n",
      "Average error at  140  epoch:   0.6762620553391057\n",
      "Accuracy:  0.831210196018219\n",
      "Average error at  141  epoch:   0.6768258892130545\n",
      "Accuracy:  0.8280254602432251\n",
      "Average error at  142  epoch:   0.6749197628491089\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  143  epoch:   0.6746545390550706\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  144  epoch:   0.6743139787772954\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  145  epoch:   0.6734957488779881\n",
      "Accuracy:  0.843949019908905\n",
      "Average error at  146  epoch:   0.6749320057254588\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  147  epoch:   0.673443162786185\n",
      "Accuracy:  0.8280254602432251\n",
      "Average error at  148  epoch:   0.6727731443729179\n",
      "Accuracy:  0.8471337556838989\n",
      "Average error at  149  epoch:   0.6732074330349642\n",
      "Accuracy:  0.824840784072876\n",
      "Average error at  150  epoch:   0.6734259496173269\n",
      "Accuracy:  0.837579607963562\n",
      "Average error at  151  epoch:   0.6714075834419032\n",
      "Accuracy:  0.8471337556838989\n",
      "Average error at  152  epoch:   0.6717048220941646\n",
      "Accuracy:  0.8471337556838989\n",
      "Average error at  153  epoch:   0.6714750356625213\n",
      "Accuracy:  0.8343949317932129\n",
      "Average error at  154  epoch:   0.6699777708915151\n",
      "Accuracy:  0.8407643437385559\n",
      "Average error at  155  epoch:   0.671701764404668\n",
      "Accuracy:  0.824840784072876\n",
      "Average error at  156  epoch:   0.6704106188063809\n",
      "Accuracy:  0.8566879034042358\n",
      "Average error at  157  epoch:   0.6699122936253051\n",
      "Accuracy:  0.8471337556838989\n",
      "Average error at  158  epoch:   0.6698256103464388\n",
      "Accuracy:  0.8535031676292419\n",
      "Average error at  159  epoch:   0.6682978507332495\n",
      "Accuracy:  0.8566879034042358\n",
      "Average error at  160  epoch:   0.6689707849111679\n",
      "Accuracy:  0.8503184914588928\n",
      "Average error at  161  epoch:   0.6702801777447578\n",
      "Accuracy:  0.843949019908905\n",
      "Average error at  162  epoch:   0.6676183208736801\n",
      "Accuracy:  0.8566879034042358\n",
      "Average error at  163  epoch:   0.6674086367699438\n",
      "Accuracy:  0.8662420511245728\n",
      "Average error at  164  epoch:   0.6673398165133677\n",
      "Accuracy:  0.8566879034042358\n",
      "Average error at  165  epoch:   0.6668787163859771\n",
      "Accuracy:  0.8598726391792297\n",
      "Average error at  166  epoch:   0.6670176918188743\n",
      "Accuracy:  0.8694267272949219\n",
      "Average error at  167  epoch:   0.6660456114268849\n",
      "Accuracy:  0.8598726391792297\n",
      "Average error at  168  epoch:   0.666709425815874\n",
      "Accuracy:  0.8503184914588928\n",
      "Average error at  169  epoch:   0.6643544813679787\n",
      "Accuracy:  0.8694267272949219\n",
      "Average error at  170  epoch:   0.6640273221643656\n",
      "Accuracy:  0.8726114630699158\n",
      "Average error at  171  epoch:   0.6657003162177736\n",
      "Accuracy:  0.8471337556838989\n",
      "Average error at  172  epoch:   0.6647132118190904\n",
      "Accuracy:  0.8535031676292419\n",
      "Average error at  173  epoch:   0.6642978020530852\n",
      "Accuracy:  0.8726114630699158\n",
      "Average error at  174  epoch:   0.664575696655798\n",
      "Accuracy:  0.8757961988449097\n",
      "Average error at  175  epoch:   0.6627177645708513\n",
      "Accuracy:  0.8662420511245728\n",
      "Average error at  176  epoch:   0.6628000623323725\n",
      "Accuracy:  0.8694267272949219\n",
      "Average error at  177  epoch:   0.6624659563978011\n",
      "Accuracy:  0.8662420511245728\n",
      "Average error at  178  epoch:   0.6619384927288355\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  179  epoch:   0.6628246368125359\n",
      "Accuracy:  0.8630573153495789\n",
      "Average error at  180  epoch:   0.6622637102889384\n",
      "Accuracy:  0.8821656107902527\n",
      "Average error at  181  epoch:   0.6623711189458137\n",
      "Accuracy:  0.8821656107902527\n",
      "Average error at  182  epoch:   0.6610136232922078\n",
      "Accuracy:  0.8757961988449097\n",
      "Average error at  183  epoch:   0.6617092982427331\n",
      "Accuracy:  0.8694267272949219\n",
      "Average error at  184  epoch:   0.6616213521747422\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  185  epoch:   0.6607571936818822\n",
      "Accuracy:  0.8853503465652466\n",
      "Average error at  186  epoch:   0.660460647286074\n",
      "Accuracy:  0.8885350227355957\n",
      "Average error at  187  epoch:   0.6595462055157867\n",
      "Accuracy:  0.8757961988449097\n",
      "Average error at  188  epoch:   0.659400650969827\n",
      "Accuracy:  0.8885350227355957\n",
      "Average error at  189  epoch:   0.6589087383687087\n",
      "Accuracy:  0.8980891704559326\n",
      "Average error at  190  epoch:   0.6592402426567079\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  191  epoch:   0.6584625680136279\n",
      "Accuracy:  0.9012739062309265\n",
      "Average error at  192  epoch:   0.6586820334786886\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  193  epoch:   0.6583470896980538\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  194  epoch:   0.6569834542946313\n",
      "Accuracy:  0.8980891704559326\n",
      "Average error at  195  epoch:   0.6578301447469\n",
      "Accuracy:  0.8980891704559326\n",
      "Average error at  196  epoch:   0.656942025004299\n",
      "Accuracy:  0.8821656107902527\n",
      "Average error at  197  epoch:   0.6575223510364148\n",
      "Accuracy:  0.8885350227355957\n",
      "Average error at  198  epoch:   0.657395058210292\n",
      "Accuracy:  0.9012739062309265\n",
      "Average error at  199  epoch:   0.6562518532851177\n",
      "Accuracy:  0.8917197585105896\n",
      "Average error at  200  epoch:   0.6566702652804047\n",
      "Accuracy:  0.8853503465652466\n",
      "Average error at  201  epoch:   0.6559922462977583\n",
      "Accuracy:  0.8917197585105896\n",
      "Average error at  202  epoch:   0.6574969770167836\n",
      "Accuracy:  0.8949044346809387\n",
      "Average error at  203  epoch:   0.6558402488267163\n",
      "Accuracy:  0.8885350227355957\n",
      "Average error at  204  epoch:   0.6552399215454697\n",
      "Accuracy:  0.9044585824012756\n",
      "Average error at  205  epoch:   0.6549966579298745\n",
      "Accuracy:  0.9076433181762695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  206  epoch:   0.6539765472927518\n",
      "Accuracy:  0.8949044346809387\n",
      "Average error at  207  epoch:   0.6557043349898508\n",
      "Accuracy:  0.8789808750152588\n",
      "Average error at  208  epoch:   0.6544216233605398\n",
      "Accuracy:  0.9203821420669556\n",
      "Average error at  209  epoch:   0.653650358073345\n",
      "Accuracy:  0.9108280539512634\n",
      "Average error at  210  epoch:   0.6544622842294939\n",
      "Accuracy:  0.9012739062309265\n",
      "Average error at  211  epoch:   0.6534876776352438\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  212  epoch:   0.6541205136598514\n",
      "Accuracy:  0.8949044346809387\n",
      "Average error at  213  epoch:   0.6528455430332982\n",
      "Accuracy:  0.9203821420669556\n",
      "Average error at  214  epoch:   0.653480650399286\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  215  epoch:   0.6530230660762479\n",
      "Accuracy:  0.9171974658966064\n",
      "Average error at  216  epoch:   0.6513436139489488\n",
      "Accuracy:  0.9203821420669556\n",
      "Average error at  217  epoch:   0.6527768581644524\n",
      "Accuracy:  0.9076433181762695\n",
      "Average error at  218  epoch:   0.6532513461336977\n",
      "Accuracy:  0.8949044346809387\n",
      "Average error at  219  epoch:   0.6534903975685671\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  220  epoch:   0.6524967314956424\n",
      "Accuracy:  0.8885350227355957\n",
      "Average error at  221  epoch:   0.6514464401929423\n",
      "Accuracy:  0.9076433181762695\n",
      "Average error at  222  epoch:   0.6514232808856231\n",
      "Accuracy:  0.9044585824012756\n",
      "Average error at  223  epoch:   0.650823172779914\n",
      "Accuracy:  0.9044585824012756\n",
      "Average error at  224  epoch:   0.6506721612373713\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  225  epoch:   0.6511138920077416\n",
      "Accuracy:  0.9267516136169434\n",
      "Average error at  226  epoch:   0.6508531485142064\n",
      "Accuracy:  0.9171974658966064\n",
      "Average error at  227  epoch:   0.6506001999684521\n",
      "Accuracy:  0.9299362897872925\n",
      "Average error at  228  epoch:   0.6500616639826248\n",
      "Accuracy:  0.9299362897872925\n",
      "Average error at  229  epoch:   0.6512966627366852\n",
      "Accuracy:  0.8917197585105896\n",
      "Average error at  230  epoch:   0.6492752489727587\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  231  epoch:   0.6494359903903629\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  232  epoch:   0.6499466205232032\n",
      "Accuracy:  0.9140127301216125\n",
      "Average error at  233  epoch:   0.6495337094019654\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  234  epoch:   0.6496363656867592\n",
      "Accuracy:  0.9267516136169434\n",
      "Average error at  235  epoch:   0.6494864932839586\n",
      "Accuracy:  0.9299362897872925\n",
      "Average error at  236  epoch:   0.6491533986556356\n",
      "Accuracy:  0.9108280539512634\n",
      "Average error at  237  epoch:   0.6482076223575752\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  238  epoch:   0.6480546546577637\n",
      "Accuracy:  0.9363057613372803\n",
      "Average error at  239  epoch:   0.6481688921986946\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  240  epoch:   0.6472845592813756\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  241  epoch:   0.6474122418312648\n",
      "Accuracy:  0.9203821420669556\n",
      "Average error at  242  epoch:   0.6468260274343892\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  243  epoch:   0.6475987568950214\n",
      "Accuracy:  0.9235668778419495\n",
      "Average error at  244  epoch:   0.6470743528265501\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  245  epoch:   0.6467754645035599\n",
      "Accuracy:  0.9299362897872925\n",
      "Average error at  246  epoch:   0.6471168111443509\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  247  epoch:   0.6471554158108208\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  248  epoch:   0.6473294036053097\n",
      "Accuracy:  0.9267516136169434\n",
      "Average error at  249  epoch:   0.6466828050885877\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  250  epoch:   0.6470967230403671\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  251  epoch:   0.645954247336817\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  252  epoch:   0.6448476170087388\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  253  epoch:   0.6458209426384721\n",
      "Accuracy:  0.9363057613372803\n",
      "Average error at  254  epoch:   0.6449205360860913\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  255  epoch:   0.6459609370244408\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  256  epoch:   0.6455654727745399\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  257  epoch:   0.6453180348389986\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  258  epoch:   0.6455617837690775\n",
      "Accuracy:  0.9299362897872925\n",
      "Average error at  259  epoch:   0.6448720007558774\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  260  epoch:   0.6447063572272429\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  261  epoch:   0.6434618673638731\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  262  epoch:   0.6454330332278837\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  263  epoch:   0.6455534792591365\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  264  epoch:   0.6436497197258796\n",
      "Accuracy:  0.9363057613372803\n",
      "Average error at  265  epoch:   0.6446191232677755\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  266  epoch:   0.6441068955942522\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  267  epoch:   0.643937788297329\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  268  epoch:   0.6432994792672023\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  269  epoch:   0.6435411731894684\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  270  epoch:   0.6432007226339176\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  271  epoch:   0.6434744180063405\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  272  epoch:   0.6427972542180163\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  273  epoch:   0.6432410839230428\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  274  epoch:   0.6431465257083301\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  275  epoch:   0.6420504060971751\n",
      "Accuracy:  0.9394904375076294\n",
      "Average error at  276  epoch:   0.6419713918309639\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  277  epoch:   0.6438217822056159\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  278  epoch:   0.6418900203894536\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  279  epoch:   0.6422591918919095\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  280  epoch:   0.6416664837630051\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  281  epoch:   0.6421444811952755\n",
      "Accuracy:  0.9363057613372803\n",
      "Average error at  282  epoch:   0.6414999462890785\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  283  epoch:   0.6418708958370223\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  284  epoch:   0.6412248770762592\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  285  epoch:   0.6424499764396377\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  286  epoch:   0.6408679404413644\n",
      "Accuracy:  0.9426751732826233\n",
      "Average error at  287  epoch:   0.6401829066541628\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  288  epoch:   0.6404461959904492\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  289  epoch:   0.6410079889769003\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  290  epoch:   0.641018506641531\n",
      "Accuracy:  0.9331210255622864\n",
      "Average error at  291  epoch:   0.64199870818412\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  292  epoch:   0.6414398203564896\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  293  epoch:   0.6400308914228845\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  294  epoch:   0.6403677071159126\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  295  epoch:   0.6409952229414342\n",
      "Accuracy:  0.9458598494529724\n",
      "Average error at  296  epoch:   0.6399821610075371\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  297  epoch:   0.6398352453425153\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  298  epoch:   0.6397168977939065\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  299  epoch:   0.6394074256371335\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  300  epoch:   0.6399941998135737\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  301  epoch:   0.6405998114229768\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  302  epoch:   0.6397667892078578\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  303  epoch:   0.6383241283124421\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  304  epoch:   0.6402379846160663\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  305  epoch:   0.6390583314745973\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  306  epoch:   0.6388849166269545\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  307  epoch:   0.6390485684914217\n",
      "Accuracy:  0.9649681448936462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  308  epoch:   0.6393040281531225\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  309  epoch:   0.6393699469838499\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  310  epoch:   0.6384769500883805\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  311  epoch:   0.6384018863513772\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  312  epoch:   0.6393351743927373\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  313  epoch:   0.6389448099043118\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  314  epoch:   0.6385806880595613\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  315  epoch:   0.6382854483422217\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  316  epoch:   0.638064829550629\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  317  epoch:   0.6385938337087282\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  318  epoch:   0.6380894805912884\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  319  epoch:   0.6386724929876669\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  320  epoch:   0.638272248917672\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  321  epoch:   0.6371278427792992\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  322  epoch:   0.6382074961860978\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  323  epoch:   0.6375790742452393\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  324  epoch:   0.637781171221089\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  325  epoch:   0.6379271027506029\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  326  epoch:   0.6369402116254441\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  327  epoch:   0.636792106414264\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  328  epoch:   0.6380181528126505\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  329  epoch:   0.6368314456198156\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  330  epoch:   0.6369454626086702\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  331  epoch:   0.6369151174022248\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  332  epoch:   0.6364500458675977\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  333  epoch:   0.6353504662870383\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  334  epoch:   0.6368455375568662\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  335  epoch:   0.6359457766887282\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  336  epoch:   0.6357346753356823\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  337  epoch:   0.6370730144943776\n",
      "Accuracy:  0.9490445852279663\n",
      "Average error at  338  epoch:   0.6354726269761269\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  339  epoch:   0.6362221587789032\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  340  epoch:   0.6361924801178614\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  341  epoch:   0.6361162019778194\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  342  epoch:   0.6369118842439732\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  343  epoch:   0.6358843817987568\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  344  epoch:   0.6358479611315165\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  345  epoch:   0.6363695060924762\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  346  epoch:   0.6350630836712505\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  347  epoch:   0.6354372945114282\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  348  epoch:   0.6356177674597367\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  349  epoch:   0.6367560027991279\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  350  epoch:   0.6355665988458321\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  351  epoch:   0.6365780152044891\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  352  epoch:   0.6355606597449156\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  353  epoch:   0.6346267891813165\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  354  epoch:   0.6356509010447934\n",
      "Accuracy:  0.9522293210029602\n",
      "Average error at  355  epoch:   0.6348551911590701\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  356  epoch:   0.6350726260246967\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  357  epoch:   0.6352308896600183\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  358  epoch:   0.6344480811556819\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  359  epoch:   0.6343377649510437\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  360  epoch:   0.6345441774853396\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  361  epoch:   0.6347975520110197\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  362  epoch:   0.6347628176590505\n",
      "Accuracy:  0.9554139971733093\n",
      "Average error at  363  epoch:   0.6342439439480677\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  364  epoch:   0.6340202807531679\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  365  epoch:   0.6343926867700618\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  366  epoch:   0.6340841439069793\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  367  epoch:   0.6345468324813514\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  368  epoch:   0.6338259296603207\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  369  epoch:   0.6346857060513527\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  370  epoch:   0.6341224539247141\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  371  epoch:   0.6339106676590631\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  372  epoch:   0.6341784685927875\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  373  epoch:   0.6334344984833046\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  374  epoch:   0.6333068264769413\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  375  epoch:   0.633982466035723\n",
      "Accuracy:  0.9585987329483032\n",
      "Average error at  376  epoch:   0.6342093608535719\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  377  epoch:   0.6336028918523956\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  378  epoch:   0.633945774984307\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  379  epoch:   0.632924685734977\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  380  epoch:   0.6337568166651547\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  381  epoch:   0.6330523196068609\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  382  epoch:   0.6333524750359321\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  383  epoch:   0.6337666496509499\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  384  epoch:   0.6339282347186779\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  385  epoch:   0.6335986068404595\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  386  epoch:   0.6332847646056889\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  387  epoch:   0.6331280399680483\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  388  epoch:   0.6320884036800479\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  389  epoch:   0.6328600568870746\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  390  epoch:   0.6327737345165522\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  391  epoch:   0.6326345215692099\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  392  epoch:   0.6324005110128007\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  393  epoch:   0.6329565522940886\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  394  epoch:   0.6326960862448207\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  395  epoch:   0.6322886705884373\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  396  epoch:   0.6322958458061921\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  397  epoch:   0.632834787013122\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  398  epoch:   0.6318768659562183\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  399  epoch:   0.6325408949457563\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  400  epoch:   0.6324258484865367\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  401  epoch:   0.6323956528934922\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  402  epoch:   0.6317473253424868\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  403  epoch:   0.6325030429103022\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  404  epoch:   0.6322729317397444\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  405  epoch:   0.6313656204657243\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  406  epoch:   0.63152482546519\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  407  epoch:   0.6316999570804661\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  408  epoch:   0.6313512291668919\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  409  epoch:   0.6318650163473598\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  410  epoch:   0.6320747248703069\n",
      "Accuracy:  0.977707028388977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  411  epoch:   0.6317743342608284\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  412  epoch:   0.630915075863369\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  413  epoch:   0.6307251563223881\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  414  epoch:   0.6315978730746062\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  415  epoch:   0.6315054565584003\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  416  epoch:   0.6316546701393306\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  417  epoch:   0.6315258220838095\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  418  epoch:   0.6323135189066706\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  419  epoch:   0.6310618035081094\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  420  epoch:   0.6308876413862298\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  421  epoch:   0.6311862111634772\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  422  epoch:   0.6311945756034592\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  423  epoch:   0.6310088135461338\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  424  epoch:   0.6303980828565736\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  425  epoch:   0.630944719740155\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  426  epoch:   0.631485628155868\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  427  epoch:   0.6311743463261066\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  428  epoch:   0.6304285802792413\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  429  epoch:   0.6311961961017242\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  430  epoch:   0.6309841744124487\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  431  epoch:   0.6307369303805104\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  432  epoch:   0.6305388690265097\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  433  epoch:   0.6302381482136208\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  434  epoch:   0.6313966557541629\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  435  epoch:   0.6304340066091756\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  436  epoch:   0.6301698606267923\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  437  epoch:   0.6304799140414181\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  438  epoch:   0.6306487444659912\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  439  epoch:   0.6312006157809695\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  440  epoch:   0.6297534639525212\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  441  epoch:   0.630337969575637\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  442  epoch:   0.6305054720261116\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  443  epoch:   0.630698058343574\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  444  epoch:   0.6304795491820807\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  445  epoch:   0.6303515653348404\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  446  epoch:   0.6296251477806294\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  447  epoch:   0.6301365076778982\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  448  epoch:   0.6302452263240255\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  449  epoch:   0.6296896151014313\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  450  epoch:   0.6299796079591893\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  451  epoch:   0.6295155181203503\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  452  epoch:   0.6298586378984299\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  453  epoch:   0.6296778673880938\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  454  epoch:   0.6290362696983818\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  455  epoch:   0.6299655252082179\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  456  epoch:   0.6294466053840531\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  457  epoch:   0.6299605335113374\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  458  epoch:   0.6292663231680891\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  459  epoch:   0.6296310266044474\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  460  epoch:   0.6300363491511864\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  461  epoch:   0.6294376907449464\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  462  epoch:   0.6287892779122677\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  463  epoch:   0.629766981515034\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  464  epoch:   0.630319044930624\n",
      "Accuracy:  0.9649681448936462\n",
      "Average error at  465  epoch:   0.6296151008546219\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  466  epoch:   0.6294127459299901\n",
      "Accuracy:  0.9713375568389893\n",
      "Average error at  467  epoch:   0.6294373947837528\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  468  epoch:   0.6295900299485169\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  469  epoch:   0.6293547625314085\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  470  epoch:   0.6293420566581645\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  471  epoch:   0.6293640861774644\n",
      "Accuracy:  0.9617834687232971\n",
      "Average error at  472  epoch:   0.6286309862802798\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  473  epoch:   0.6301182701346668\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  474  epoch:   0.6295619279949028\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  475  epoch:   0.6293746406696302\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  476  epoch:   0.6289782733096039\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  477  epoch:   0.6291916967678167\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  478  epoch:   0.6286532988632811\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  479  epoch:   0.6295910391343574\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  480  epoch:   0.6288777162689789\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  481  epoch:   0.628533355947837\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  482  epoch:   0.6292404354096189\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  483  epoch:   0.6289651507196234\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  484  epoch:   0.6286281718440709\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  485  epoch:   0.6288211988768309\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  486  epoch:   0.6288727830920156\n",
      "Accuracy:  0.9745222926139832\n",
      "Average error at  487  epoch:   0.6284511267533427\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  488  epoch:   0.6284237855406338\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  489  epoch:   0.6285844185368226\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  490  epoch:   0.6284306999423066\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  491  epoch:   0.6283643073913574\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  492  epoch:   0.6293551756031956\n",
      "Accuracy:  0.9681528806686401\n",
      "Average error at  493  epoch:   0.6287829384625458\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  494  epoch:   0.6285846742449201\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  495  epoch:   0.6289692758617613\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  496  epoch:   0.6278183783231057\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  497  epoch:   0.6288911341991749\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  498  epoch:   0.6284013832800344\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  499  epoch:   0.6284514618922097\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  500  epoch:   0.628167561555654\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  501  epoch:   0.6281827576044915\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  502  epoch:   0.6279951324299663\n",
      "Accuracy:  0.9968152642250061\n",
      "Average error at  503  epoch:   0.6277704429980164\n",
      "Accuracy:  0.9968152642250061\n",
      "Average error at  504  epoch:   0.6282316128237899\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  505  epoch:   0.6281669031245419\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  506  epoch:   0.6286965291684959\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  507  epoch:   0.6283299059411886\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  508  epoch:   0.6279304637668787\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  509  epoch:   0.6273581295460743\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  510  epoch:   0.6277089315091192\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  511  epoch:   0.628137221225259\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  512  epoch:   0.6276905689448066\n",
      "Accuracy:  0.9904458522796631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error at  513  epoch:   0.6276214243889692\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  514  epoch:   0.6274791668304316\n",
      "Accuracy:  0.993630588054657\n",
      "Average error at  515  epoch:   0.6279556626483653\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  516  epoch:   0.6277920163152232\n",
      "Accuracy:  0.9968152642250061\n",
      "Average error at  517  epoch:   0.6275285822095505\n",
      "Accuracy:  0.9808917045593262\n",
      "Average error at  518  epoch:   0.6276985836960352\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  519  epoch:   0.6277834693720825\n",
      "Accuracy:  0.987261176109314\n",
      "Average error at  520  epoch:   0.6285016477597385\n",
      "Accuracy:  0.977707028388977\n",
      "Average error at  521  epoch:   0.6281724810115026\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  522  epoch:   0.6275500591761465\n",
      "Accuracy:  0.9840764403343201\n",
      "Average error at  523  epoch:   0.6273720354207218\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  524  epoch:   0.6277424952576038\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  525  epoch:   0.6274511880609587\n",
      "Accuracy:  0.9968152642250061\n",
      "Average error at  526  epoch:   0.6270133867768146\n",
      "Accuracy:  0.9968152642250061\n",
      "Average error at  527  epoch:   0.6276692185553993\n",
      "Accuracy:  0.9904458522796631\n",
      "Average error at  528  epoch:   0.6274363520608337\n",
      "Accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, l = train(5000, batched_train, model, optimizer, loss)\n",
    "#at first it errors and i tink it has to do with reshaping of the predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = accuracy(p.view(-1), l.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6939e-03, 1.6656e-03, 2.3245e-02, 1.0905e-02, 1.2626e-02, 8.4715e-01,\n",
       "        9.7049e-01, 1.0227e-02, 1.3083e-03, 6.4956e-03, 7.6901e-03, 9.1521e-01,\n",
       "        8.4133e-01, 3.1540e-03, 1.1238e-02, 1.4219e-02, 3.2357e-02, 3.4382e-04,\n",
       "        7.2503e-03, 9.4239e-01, 8.8206e-03, 2.4090e-03, 9.4538e-01, 7.1130e-03,\n",
       "        3.7919e-03, 1.0887e-02, 5.1240e-02, 9.2320e-01, 6.6653e-03, 8.0599e-03,\n",
       "        1.1952e-03, 1.1303e-02, 1.1359e-02, 2.3539e-02, 9.7195e-01, 4.4480e-03,\n",
       "        3.8074e-03, 2.6693e-03, 9.5805e-01, 6.9856e-03, 7.1895e-03, 7.4740e-03,\n",
       "        4.1757e-02, 9.8240e-01, 4.7571e-03, 9.6093e-01, 3.0062e-03, 5.4934e-03,\n",
       "        5.4368e-03, 9.7911e-01, 1.5566e-02, 9.7687e-01, 2.5183e-03, 4.8171e-03,\n",
       "        5.4139e-02, 2.6068e-03, 4.1067e-03, 4.6368e-03, 1.5402e-02, 3.0629e-02,\n",
       "        9.9466e-01, 1.7961e-02, 5.9592e-03, 8.9300e-01, 9.3331e-01, 9.8773e-01,\n",
       "        2.2084e-02, 3.2353e-02, 3.4456e-03, 1.2693e-02, 6.4972e-03, 8.0378e-04,\n",
       "        9.8435e-03, 2.2193e-02, 8.7662e-03, 3.5958e-02, 5.6011e-03, 9.4850e-03,\n",
       "        3.8758e-03, 6.7271e-04, 4.8659e-04, 9.4255e-01, 2.6660e-03, 1.5081e-02,\n",
       "        5.9434e-03, 9.8759e-01, 9.7138e-01, 2.6910e-03, 9.8986e-01, 4.2501e-03,\n",
       "        1.7788e-02, 2.5114e-02, 2.1859e-02, 4.2348e-03, 2.2608e-03, 7.7325e-03,\n",
       "        1.4496e-02, 4.0149e-03, 9.2870e-03, 5.3706e-02, 4.2511e-03, 8.0374e-03,\n",
       "        8.7856e-03, 6.3564e-03, 9.9256e-01, 9.8490e-03, 7.5665e-01, 2.5666e-03,\n",
       "        6.7919e-02, 1.6567e-02, 4.7405e-03, 9.8389e-01, 3.2951e-03, 8.0128e-03,\n",
       "        5.4493e-03, 8.6177e-03, 9.8147e-03, 2.1314e-03, 9.5407e-01, 2.9370e-03,\n",
       "        3.4468e-03, 2.4507e-02, 5.3043e-02, 1.6396e-02, 5.8730e-03, 2.6010e-03,\n",
       "        8.4854e-03, 9.5871e-01, 4.3616e-03, 1.3694e-02, 9.2410e-01, 9.3290e-01,\n",
       "        9.8075e-03, 9.8089e-01, 1.1752e-03, 9.1803e-03, 6.4115e-03, 2.1377e-03,\n",
       "        3.2963e-03, 9.4991e-01, 4.0281e-03, 3.3449e-02, 9.9146e-01, 4.8973e-03,\n",
       "        1.1214e-02, 9.9466e-01, 4.4468e-03, 9.0623e-03, 8.7774e-03, 3.6604e-03,\n",
       "        7.9758e-01, 5.9934e-03, 5.9947e-03, 2.2997e-03, 9.8906e-01, 9.3758e-03,\n",
       "        1.8403e-02, 1.9194e-02, 7.1135e-03, 9.5899e-01, 9.7782e-01, 9.9904e-01,\n",
       "        7.3107e-03, 2.9407e-02, 3.5934e-03, 1.1883e-02, 9.8938e-03, 3.0705e-02,\n",
       "        4.2488e-03, 3.6488e-03, 8.2281e-01, 3.2318e-02, 9.9023e-01, 6.6977e-02,\n",
       "        9.8153e-01, 1.1786e-02, 4.1600e-04, 9.7398e-01, 4.8600e-03, 9.8795e-01,\n",
       "        1.0224e-03, 7.2790e-03, 9.7338e-01, 2.8803e-03, 1.7059e-03, 1.0377e-02,\n",
       "        8.6454e-01, 9.9876e-01, 1.6189e-03, 6.8439e-03, 7.2788e-03, 5.4948e-03,\n",
       "        1.2676e-02, 1.1061e-03, 1.0230e-02, 1.4666e-01, 2.3965e-04, 2.1651e-02,\n",
       "        1.9770e-02, 3.4600e-03, 6.8380e-02, 1.5528e-02, 8.2709e-03, 5.6560e-03,\n",
       "        9.7749e-01, 6.5856e-03, 5.1612e-02, 8.8018e-03, 1.3168e-02, 2.4848e-02,\n",
       "        1.0143e-02, 1.8532e-03, 8.4113e-01, 4.3252e-03, 1.1420e-02, 4.5342e-03,\n",
       "        6.2080e-03, 4.7919e-02, 9.4707e-01, 1.2411e-01, 8.0596e-03, 3.0247e-03,\n",
       "        5.5705e-03, 9.2108e-04, 3.4117e-03, 9.4533e-03, 4.9163e-03, 3.9094e-03,\n",
       "        4.6832e-02, 3.9678e-02, 1.0812e-03, 1.7501e-03, 7.8251e-01, 9.4966e-01,\n",
       "        1.1803e-02, 6.7934e-03, 6.2927e-03, 8.4479e-01, 6.2753e-03, 7.6565e-03,\n",
       "        9.6297e-01, 3.2737e-02, 2.3708e-03, 9.7419e-01, 1.0230e-03, 2.0607e-01,\n",
       "        9.3719e-01, 9.5354e-01, 2.7177e-03, 9.0627e-01, 9.9595e-01, 1.1538e-02,\n",
       "        1.2712e-02, 5.7198e-03, 1.2837e-02, 2.6034e-01, 9.8594e-01, 9.9570e-01,\n",
       "        8.5541e-02, 3.8085e-03, 7.1344e-03, 4.0503e-02, 1.3775e-02, 1.7886e-02,\n",
       "        1.5898e-02, 2.1656e-02, 8.3752e-01, 4.4190e-02, 1.7284e-02, 9.3643e-01,\n",
       "        5.6453e-03, 7.5188e-03, 6.1889e-03, 5.3195e-03, 2.4646e-02, 1.4242e-02,\n",
       "        4.0385e-03, 3.9814e-03, 1.1013e-03, 2.1652e-03, 2.5719e-02, 7.4800e-03,\n",
       "        3.8343e-02, 6.4057e-03, 3.2652e-03, 6.2217e-03, 1.3997e-02, 1.3083e-02,\n",
       "        1.2781e-03, 9.8562e-01, 7.6833e-01, 3.7049e-02, 7.4586e-03, 6.4424e-03,\n",
       "        9.8649e-01, 1.0217e-02, 1.1745e-02, 1.1501e-02, 1.7328e-02, 9.9793e-01,\n",
       "        6.0769e-02, 1.4963e-02, 3.9203e-03, 8.7209e-01, 5.4642e-03, 3.3652e-03,\n",
       "        3.9435e-02, 3.9928e-02, 9.7341e-01, 4.6889e-03, 4.4595e-03, 3.6148e-02,\n",
       "        3.7734e-02, 9.2310e-01], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Accuracy:  1.0\n",
      "Accuracy:  1.0\n",
      "Average error at  0  epoch:   0.612527072429657\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "end of test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pl, ll = test(1, batched_test, model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0105],\n",
       "         [0.0096],\n",
       "         [0.0145],\n",
       "         [0.9825],\n",
       "         [0.0163],\n",
       "         [0.9669],\n",
       "         [0.0073],\n",
       "         [0.9887],\n",
       "         [0.0111],\n",
       "         [0.9692],\n",
       "         [0.0334],\n",
       "         [0.0112],\n",
       "         [0.0065],\n",
       "         [0.0099],\n",
       "         [0.0091],\n",
       "         [0.0113],\n",
       "         [0.0114],\n",
       "         [0.0105],\n",
       "         [0.0145],\n",
       "         [0.0105],\n",
       "         [0.0073],\n",
       "         [0.9819],\n",
       "         [0.9175],\n",
       "         [0.0066],\n",
       "         [0.0044],\n",
       "         [0.0342],\n",
       "         [0.0122],\n",
       "         [0.0156],\n",
       "         [0.0170],\n",
       "         [0.0103],\n",
       "         [0.0260],\n",
       "         [0.0130],\n",
       "         [0.0073],\n",
       "         [0.9700],\n",
       "         [0.0139],\n",
       "         [0.9562],\n",
       "         [0.0049],\n",
       "         [0.0154],\n",
       "         [0.0130],\n",
       "         [0.0095],\n",
       "         [0.0108],\n",
       "         [0.0114],\n",
       "         [0.0143],\n",
       "         [0.0099],\n",
       "         [0.0103],\n",
       "         [0.0073],\n",
       "         [0.9771],\n",
       "         [0.9740],\n",
       "         [0.0099],\n",
       "         [0.0049],\n",
       "         [0.0097],\n",
       "         [0.0266],\n",
       "         [0.0594],\n",
       "         [0.0069],\n",
       "         [0.9825],\n",
       "         [0.0097],\n",
       "         [0.0076],\n",
       "         [0.0164],\n",
       "         [0.0130],\n",
       "         [0.9866],\n",
       "         [0.0103],\n",
       "         [0.0164],\n",
       "         [0.0069],\n",
       "         [0.0082],\n",
       "         [0.9692],\n",
       "         [0.0114],\n",
       "         [0.0291],\n",
       "         [0.0099],\n",
       "         [0.0104],\n",
       "         [0.9740],\n",
       "         [0.0073],\n",
       "         [0.0137],\n",
       "         [0.0117],\n",
       "         [0.0138],\n",
       "         [0.0368],\n",
       "         [0.0122],\n",
       "         [0.0073],\n",
       "         [0.9782],\n",
       "         [0.0139],\n",
       "         [0.0097],\n",
       "         [0.0061],\n",
       "         [0.0065],\n",
       "         [0.0122],\n",
       "         [0.0065],\n",
       "         [0.0103],\n",
       "         [0.0117],\n",
       "         [0.0065],\n",
       "         [0.0104],\n",
       "         [0.0087],\n",
       "         [0.0137],\n",
       "         [0.9600],\n",
       "         [0.9626],\n",
       "         [0.0105],\n",
       "         [0.0049],\n",
       "         [0.9834],\n",
       "         [0.0184],\n",
       "         [0.0468],\n",
       "         [0.9220],\n",
       "         [0.0176],\n",
       "         [0.0099],\n",
       "         [0.0118],\n",
       "         [0.0141],\n",
       "         [0.9650],\n",
       "         [0.9779],\n",
       "         [0.0143],\n",
       "         [0.9784],\n",
       "         [0.9585],\n",
       "         [0.0108],\n",
       "         [0.9787],\n",
       "         [0.9812],\n",
       "         [0.9403],\n",
       "         [0.9584],\n",
       "         [0.0443],\n",
       "         [0.0099],\n",
       "         [0.9887],\n",
       "         [0.0069],\n",
       "         [0.0129],\n",
       "         [0.0084],\n",
       "         [0.0170],\n",
       "         [0.0097],\n",
       "         [0.0122],\n",
       "         [0.9837],\n",
       "         [0.9810],\n",
       "         [0.0104],\n",
       "         [0.0108],\n",
       "         [0.9385],\n",
       "         [0.9700],\n",
       "         [0.0114],\n",
       "         [0.9669],\n",
       "         [0.0099],\n",
       "         [0.0140],\n",
       "         [0.0106],\n",
       "         [0.9600],\n",
       "         [0.0227],\n",
       "         [0.0066],\n",
       "         [0.0069],\n",
       "         [0.0106],\n",
       "         [0.0105],\n",
       "         [0.0099],\n",
       "         [0.0325],\n",
       "         [0.9816],\n",
       "         [0.9573],\n",
       "         [0.0432],\n",
       "         [0.9834],\n",
       "         [0.0140],\n",
       "         [0.0099],\n",
       "         [0.0271],\n",
       "         [0.0042],\n",
       "         [0.0127],\n",
       "         [0.9907],\n",
       "         [0.0166],\n",
       "         [0.0206],\n",
       "         [0.0091],\n",
       "         [0.0069],\n",
       "         [0.0091],\n",
       "         [0.0069],\n",
       "         [0.0122],\n",
       "         [0.0141],\n",
       "         [0.0140],\n",
       "         [0.9733],\n",
       "         [0.0102],\n",
       "         [0.9740],\n",
       "         [0.0137],\n",
       "         [0.0184],\n",
       "         [0.0073],\n",
       "         [0.0384],\n",
       "         [0.9817],\n",
       "         [0.9781],\n",
       "         [0.9576],\n",
       "         [0.0099],\n",
       "         [0.9731],\n",
       "         [0.0122],\n",
       "         [0.0141],\n",
       "         [0.0301],\n",
       "         [0.0272],\n",
       "         [0.0034],\n",
       "         [0.0163],\n",
       "         [0.0131],\n",
       "         [0.0310],\n",
       "         [0.0103],\n",
       "         [0.0117],\n",
       "         [0.0082],\n",
       "         [0.0141],\n",
       "         [0.9770],\n",
       "         [0.0143],\n",
       "         [0.0107],\n",
       "         [0.0105],\n",
       "         [0.9759],\n",
       "         [0.0105],\n",
       "         [0.9770],\n",
       "         [0.0066],\n",
       "         [0.0069],\n",
       "         [0.0056],\n",
       "         [0.9834],\n",
       "         [0.0103],\n",
       "         [0.9819],\n",
       "         [0.0122],\n",
       "         [0.0119],\n",
       "         [0.0117],\n",
       "         [0.0065],\n",
       "         [0.9917],\n",
       "         [0.9781],\n",
       "         [0.9779],\n",
       "         [0.0158],\n",
       "         [0.0168],\n",
       "         [0.0091],\n",
       "         [0.8668],\n",
       "         [0.0099],\n",
       "         [0.0066],\n",
       "         [0.0097],\n",
       "         [0.0117],\n",
       "         [0.9195],\n",
       "         [0.0091],\n",
       "         [0.9873],\n",
       "         [0.0081],\n",
       "         [0.0226],\n",
       "         [0.0091],\n",
       "         [0.0189],\n",
       "         [0.9856],\n",
       "         [0.0034],\n",
       "         [0.0074],\n",
       "         [0.9673],\n",
       "         [0.0141],\n",
       "         [0.0072],\n",
       "         [0.9727],\n",
       "         [0.0122],\n",
       "         [0.0105],\n",
       "         [0.9825],\n",
       "         [0.0042],\n",
       "         [0.0431],\n",
       "         [0.0141],\n",
       "         [0.0099],\n",
       "         [0.0122],\n",
       "         [0.0196],\n",
       "         [0.0055],\n",
       "         [0.0118],\n",
       "         [0.0082],\n",
       "         [0.0105],\n",
       "         [0.0098],\n",
       "         [0.0049],\n",
       "         [0.0104],\n",
       "         [0.9869],\n",
       "         [0.0065],\n",
       "         [0.0085],\n",
       "         [0.9809],\n",
       "         [0.0085],\n",
       "         [0.0066],\n",
       "         [0.0087],\n",
       "         [0.0127],\n",
       "         [0.0201],\n",
       "         [0.0145],\n",
       "         [0.9296],\n",
       "         [0.0143],\n",
       "         [0.0271],\n",
       "         [0.9755],\n",
       "         [0.0073],\n",
       "         [0.0099],\n",
       "         [0.0122],\n",
       "         [0.0049],\n",
       "         [0.0172],\n",
       "         [0.9819],\n",
       "         [0.0065],\n",
       "         [0.0105],\n",
       "         [0.0122],\n",
       "         [0.0270],\n",
       "         [0.0200],\n",
       "         [0.0184],\n",
       "         [0.0166],\n",
       "         [0.0069],\n",
       "         [0.0122],\n",
       "         [0.9755],\n",
       "         [0.0235],\n",
       "         [0.0384],\n",
       "         [0.9859],\n",
       "         [0.7551],\n",
       "         [0.9786],\n",
       "         [0.0130],\n",
       "         [0.9733],\n",
       "         [0.0106],\n",
       "         [0.0106],\n",
       "         [0.0107],\n",
       "         [0.0122],\n",
       "         [0.0105],\n",
       "         [0.0097],\n",
       "         [0.0122],\n",
       "         [0.0103],\n",
       "         [0.9573],\n",
       "         [0.0189],\n",
       "         [0.0131],\n",
       "         [0.0073],\n",
       "         [0.0105],\n",
       "         [0.0105],\n",
       "         [0.9600],\n",
       "         [0.0105],\n",
       "         [0.0122],\n",
       "         [0.0079],\n",
       "         [0.9819],\n",
       "         [0.9834],\n",
       "         [0.0069],\n",
       "         [0.0384],\n",
       "         [0.0091],\n",
       "         [0.0103],\n",
       "         [0.0168],\n",
       "         [0.9798],\n",
       "         [0.0149],\n",
       "         [0.0066],\n",
       "         [0.0156],\n",
       "         [0.0091],\n",
       "         [0.0171],\n",
       "         [0.0065],\n",
       "         [0.9819],\n",
       "         [0.9692],\n",
       "         [0.0059],\n",
       "         [0.0099]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0170],\n",
       "         [0.9740],\n",
       "         [0.0075],\n",
       "         [0.0097],\n",
       "         [0.0177],\n",
       "         [0.0082],\n",
       "         [0.0272],\n",
       "         [0.0046],\n",
       "         [0.0240],\n",
       "         [0.0122],\n",
       "         [0.0114],\n",
       "         [0.0154],\n",
       "         [0.9779],\n",
       "         [0.0119],\n",
       "         [0.0122],\n",
       "         [0.0189],\n",
       "         [0.0201],\n",
       "         [0.0183],\n",
       "         [0.0095],\n",
       "         [0.0468],\n",
       "         [0.9654],\n",
       "         [0.0184],\n",
       "         [0.9770],\n",
       "         [0.0431],\n",
       "         [0.9783],\n",
       "         [0.0049],\n",
       "         [0.0097],\n",
       "         [0.0168],\n",
       "         [0.0073],\n",
       "         [0.0099],\n",
       "         [0.0132],\n",
       "         [0.9864],\n",
       "         [0.0104],\n",
       "         [0.0131],\n",
       "         [0.9847],\n",
       "         [0.0118],\n",
       "         [0.0202],\n",
       "         [0.9740],\n",
       "         [0.9834],\n",
       "         [0.0300],\n",
       "         [0.0122],\n",
       "         [0.0130],\n",
       "         [0.0163],\n",
       "         [0.0123],\n",
       "         [0.0174],\n",
       "         [0.9825],\n",
       "         [0.0201],\n",
       "         [0.0121],\n",
       "         [0.0156],\n",
       "         [0.0328],\n",
       "         [0.0097],\n",
       "         [0.0121],\n",
       "         [0.0043],\n",
       "         [0.0085],\n",
       "         [0.9794],\n",
       "         [0.0099],\n",
       "         [0.0200],\n",
       "         [0.0230],\n",
       "         [0.9767],\n",
       "         [0.0105],\n",
       "         [0.0130],\n",
       "         [0.0149],\n",
       "         [0.9729],\n",
       "         [0.0105],\n",
       "         [0.0268],\n",
       "         [0.0074],\n",
       "         [0.0214],\n",
       "         [0.0107],\n",
       "         [0.0112],\n",
       "         [0.9649],\n",
       "         [0.0061],\n",
       "         [0.9740],\n",
       "         [0.0189],\n",
       "         [0.9644],\n",
       "         [0.0141],\n",
       "         [0.0106],\n",
       "         [0.9779],\n",
       "         [0.0382],\n",
       "         [0.0316],\n",
       "         [0.0099],\n",
       "         [0.0071],\n",
       "         [0.0065],\n",
       "         [0.0103],\n",
       "         [0.9825],\n",
       "         [0.9497],\n",
       "         [0.9843],\n",
       "         [0.0091],\n",
       "         [0.0122],\n",
       "         [0.0107],\n",
       "         [0.9692],\n",
       "         [0.0102],\n",
       "         [0.0117],\n",
       "         [0.0127],\n",
       "         [0.0069],\n",
       "         [0.0184],\n",
       "         [0.0127],\n",
       "         [0.0107],\n",
       "         [0.0107],\n",
       "         [0.9847],\n",
       "         [0.0069],\n",
       "         [0.0099],\n",
       "         [0.0069],\n",
       "         [0.9577],\n",
       "         [0.9573],\n",
       "         [0.0148],\n",
       "         [0.9766],\n",
       "         [0.0105],\n",
       "         [0.0069],\n",
       "         [0.0105],\n",
       "         [0.9669],\n",
       "         [0.0156],\n",
       "         [0.0072],\n",
       "         [0.0066],\n",
       "         [0.0156],\n",
       "         [0.0065],\n",
       "         [0.0069],\n",
       "         [0.0051],\n",
       "         [0.9695],\n",
       "         [0.0088],\n",
       "         [0.9669],\n",
       "         [0.0140],\n",
       "         [0.0155],\n",
       "         [0.0117],\n",
       "         [0.0118],\n",
       "         [0.0091],\n",
       "         [0.9824],\n",
       "         [0.0097],\n",
       "         [0.9497],\n",
       "         [0.0214],\n",
       "         [0.0061],\n",
       "         [0.9779],\n",
       "         [0.0049],\n",
       "         [0.0594],\n",
       "         [0.9472],\n",
       "         [0.0141],\n",
       "         [0.0066],\n",
       "         [0.0240],\n",
       "         [0.0185],\n",
       "         [0.0095],\n",
       "         [0.0105],\n",
       "         [0.0042],\n",
       "         [0.0099],\n",
       "         [0.0169],\n",
       "         [0.0131],\n",
       "         [0.0069],\n",
       "         [0.0073],\n",
       "         [0.0251],\n",
       "         [0.0117],\n",
       "         [0.0113],\n",
       "         [0.0104],\n",
       "         [0.0065],\n",
       "         [0.0117],\n",
       "         [0.0087],\n",
       "         [0.0226],\n",
       "         [0.0069],\n",
       "         [0.9741],\n",
       "         [0.0143],\n",
       "         [0.0103],\n",
       "         [0.9741],\n",
       "         [0.0106],\n",
       "         [0.0154],\n",
       "         [0.0196],\n",
       "         [0.0300],\n",
       "         [0.9601],\n",
       "         [0.0099],\n",
       "         [0.9869],\n",
       "         [0.9317],\n",
       "         [0.0201],\n",
       "         [0.0065],\n",
       "         [0.0072],\n",
       "         [0.0069],\n",
       "         [0.0240],\n",
       "         [0.9211],\n",
       "         [0.9770],\n",
       "         [0.0116],\n",
       "         [0.0065],\n",
       "         [0.0106],\n",
       "         [0.0114],\n",
       "         [0.0166],\n",
       "         [0.0091],\n",
       "         [0.9650],\n",
       "         [0.9758],\n",
       "         [0.0069],\n",
       "         [0.0073],\n",
       "         [0.0125],\n",
       "         [0.0140],\n",
       "         [0.0122],\n",
       "         [0.0232],\n",
       "         [0.9740],\n",
       "         [0.9860],\n",
       "         [0.0103],\n",
       "         [0.0137],\n",
       "         [0.9784],\n",
       "         [0.9709],\n",
       "         [0.0167],\n",
       "         [0.0105],\n",
       "         [0.0271],\n",
       "         [0.9727],\n",
       "         [0.0184],\n",
       "         [0.9580],\n",
       "         [0.0069],\n",
       "         [0.0079],\n",
       "         [0.0105],\n",
       "         [0.0073],\n",
       "         [0.0127],\n",
       "         [0.9824],\n",
       "         [0.9732],\n",
       "         [0.0091],\n",
       "         [0.0107],\n",
       "         [0.0103],\n",
       "         [0.0105],\n",
       "         [0.0049],\n",
       "         [0.0219],\n",
       "         [0.0103],\n",
       "         [0.0069],\n",
       "         [0.0051],\n",
       "         [0.0226],\n",
       "         [0.0105],\n",
       "         [0.0232],\n",
       "         [0.9779],\n",
       "         [0.0184],\n",
       "         [0.0122],\n",
       "         [0.0118],\n",
       "         [0.0341],\n",
       "         [0.0255],\n",
       "         [0.0214],\n",
       "         [0.0073],\n",
       "         [0.9834],\n",
       "         [0.0066],\n",
       "         [0.9669],\n",
       "         [0.0099],\n",
       "         [0.0119],\n",
       "         [0.0073],\n",
       "         [0.0136],\n",
       "         [0.0122],\n",
       "         [0.0105],\n",
       "         [0.9779],\n",
       "         [0.9819],\n",
       "         [0.9497],\n",
       "         [0.0073],\n",
       "         [0.9769],\n",
       "         [0.0091],\n",
       "         [0.9783],\n",
       "         [0.0119],\n",
       "         [0.9733],\n",
       "         [0.0077],\n",
       "         [0.0137],\n",
       "         [0.0141],\n",
       "         [0.0122],\n",
       "         [0.0141],\n",
       "         [0.0106],\n",
       "         [0.0184],\n",
       "         [0.0105],\n",
       "         [0.0152],\n",
       "         [0.0069],\n",
       "         [0.9769],\n",
       "         [0.9825],\n",
       "         [0.0137],\n",
       "         [0.9834],\n",
       "         [0.9671],\n",
       "         [0.0069],\n",
       "         [0.0171],\n",
       "         [0.0266],\n",
       "         [0.0105],\n",
       "         [0.0087],\n",
       "         [0.0099],\n",
       "         [0.0138],\n",
       "         [0.0304],\n",
       "         [0.9653],\n",
       "         [0.0150],\n",
       "         [0.0069],\n",
       "         [0.0069],\n",
       "         [0.0107],\n",
       "         [0.0065],\n",
       "         [0.0300],\n",
       "         [0.0091],\n",
       "         [0.0168],\n",
       "         [0.9783],\n",
       "         [0.0069],\n",
       "         [0.0122],\n",
       "         [0.9654],\n",
       "         [0.0069],\n",
       "         [0.0083],\n",
       "         [0.0384],\n",
       "         [0.0057],\n",
       "         [0.0105],\n",
       "         [0.0107],\n",
       "         [0.0056],\n",
       "         [0.0185],\n",
       "         [0.0106],\n",
       "         [0.9700],\n",
       "         [0.0097],\n",
       "         [0.0057],\n",
       "         [0.9573],\n",
       "         [0.0130],\n",
       "         [0.0061],\n",
       "         [0.0103],\n",
       "         [0.9601],\n",
       "         [0.0116],\n",
       "         [0.0113],\n",
       "         [0.0189],\n",
       "         [0.0081],\n",
       "         [0.0122],\n",
       "         [0.0447],\n",
       "         [0.0141],\n",
       "         [0.9793],\n",
       "         [0.0073],\n",
       "         [0.0069],\n",
       "         [0.0384],\n",
       "         [0.0103],\n",
       "         [0.0103],\n",
       "         [0.0097],\n",
       "         [0.0103],\n",
       "         [0.0251]], grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(pred, actual):\n",
    "    #print(pred)\n",
    "    pred = torch.tensor([1.0 if i >= 0.7 else 0.0 for i in pred])\n",
    "    #print('preddd: ', pred)\n",
    "    #actual = torch.tensor([1 if i >= 7.0 else 0 for i in actual])\n",
    "    same = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == actual[i]:\n",
    "            same.append(1)\n",
    "        else:\n",
    "            same.append(0)\n",
    "    same = torch.tensor(same)\n",
    "    error = 0\n",
    "    for i in same:\n",
    "        if i == 0:\n",
    "            error += 1\n",
    "    #print(pred)\n",
    "    #print(actual)\n",
    "    #print(same)\n",
    "    print('out of ', len(actual), ' predicted values, the miss predicted values are: ', error)\n",
    "    print('Thus making the accuracy: ', (sum(same)/len(same)).item())\n",
    "    print('Where Zero appears is the data that was miss predicted')\n",
    "    return same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of  314  predicted values, the miss predicted values are:  0\n",
      "Thus making the accuracy:  1.0\n",
      "Where Zero appears is the data that was miss predicted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp(pl[1], ll[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_train:\n",
    "    batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = batched_train.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_tensor = batched_train.type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform(df\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "transform(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_train[:][:][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_record = batched_train[:, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
