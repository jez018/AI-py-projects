{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#imports necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#this below code streams the data from the api\n",
    "\n",
    "#This to be sent to httpAPI as a parameter\n",
    "url = 'https://newsapi.org/v2/top-headlines?country=ng&apiKey=002648d343624364a759bffc6ec6c009'\n",
    "\n",
    "#This function returns news data in json format\n",
    "def getJsonNewsData(httpAPI):\n",
    "    response = requests.get(httpAPI)\n",
    "    print(response)\n",
    "    jsonNewsData = response.json() #this converts the response to a json format\n",
    "    return jsonNewsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonToCsvNormalizer(jsonObject):\n",
    "    normalized_json = json_normalize(json_object, record_path = ['articles'])\n",
    "    return normalized_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function converts the json data and store it in a csv file\n",
    "#check this later for bug both here and cell 66 of news app\n",
    "def convAndStoreInCSV(json_object):\n",
    "    normalized_json = json_normalize(json_object, record_path = ['articles'])\n",
    "    if(os.path.getsize('news_data.csv') == 0):\n",
    "        normalized_json.to_csv(r'news_data.csv')\n",
    "        return True\n",
    "    else:\n",
    "        os.remove('news_data.csv') #delete any existing file with the following name then...\n",
    "        normalized_json.to_csv(r'news_data.csv')#It recreate a file with the name and save normalized_json to it.\n",
    "        return True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This might be more efficient than the one above but it seems they do the same thing (to recheck)\n",
    "#It collects, converts and stores the data in .csv and .json format files \n",
    "def collAndStore(jsonObject):\n",
    "    normalized_json = json_normalize(json_object, record_path = ['articles'])\n",
    "    \n",
    "    #saves the data to a csv file\n",
    "    if(os.path.exists('news_data.csv') == True):\n",
    "        os.remove('news_data.csv')\n",
    "        normalized_json.to_csv('news_data.csv')\n",
    "    elif(os.path.exists('news_data.csv') == False):\n",
    "        normalized_json.to_csv('news_data.csv')\n",
    "        \n",
    "    if(os.path.exists('json_news_data.json') == True):\n",
    "        os.remove('json_news_data.json')\n",
    "        pd.read_csv(r'news_data.csv').to_json(r'json_news_data.json') #this converts the file to a json format\n",
    "    elif(os.path.exists('json_news_data.json') == False):\n",
    "        pd.read_csv(r'news_data.csv').to_json(r'json_news_data.json') #this converts the file to a json format\n",
    "        \n",
    "    the_json_type_data = pd.read_json(r'json_news_data.json')\n",
    "    print(the_json_type_data)\n",
    "    \n",
    "    #here the code to send the data to the phone should be written here! \n",
    "    sendDataToPhone(normalized_json)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function sends the json data to the phone\n",
    "def sendDataToPhone(data):\n",
    "    #The code to send the data to the phone goes here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT DOC:\n",
    "#This function is used for loading previous csv data from file and then appending data to it,\n",
    "#but the data is in json format\n",
    "def loadAndAppend(jsonObject):\n",
    "    #This converts new json to csv data\n",
    "    new_normalized_csv = jsonToCsvNormalizer(jsonObject)\n",
    "    \n",
    "    #this loads and appends the data\n",
    "    original = pd.read_csv('news_data.csv')\n",
    "    new_normalized_csv[\"ID\"] = 'NaN' #this creates a necessary column for ID in the new pasrsed in df\n",
    "    new_normalized_csv[\"Likes\"] = 'NaN' #this creates a necessary column for Likes in the new pasrsed in df\n",
    "    original.append(new_normalized_csv)\n",
    "    original.drop(original.columns[0], axis=1) #this removes the previous unwanted indexing\n",
    "    \n",
    "    #generates and assign unique IDs for each row of the df\n",
    "    assignID(original)\n",
    "    \n",
    "    #del the prev file\n",
    "    os.remove('news_data.csv')\n",
    "    \n",
    "    #create a new df thus just like overwriting it\n",
    "    original.to_csv('news_data.csv')\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT DOC:\n",
    "#Whilst using this function one must pass in atleast an empty list or a list in other to avoid catastrophic error. \n",
    "\n",
    "def uniqueIdGen(listOfPrevIDs):\n",
    "    prevIDs = listOfPrevIDs\n",
    "    \n",
    "    #This block of code generates the id\n",
    "    generator = [random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('1234567890'),random.choice('ABCDEF'),random.choice('ABCDEF'),random.choice('ABCDEF'),random.choice('ABCDEF'),random.choice('ABCDEF'),random.choice('ABCDEF')]\n",
    "    random.shuffle(generator)\n",
    "    uniqueID = ''.join(generator)\n",
    "    \n",
    "    if uniqueID not in prevIDs:\n",
    "        listOfPrevIDs.append(uniqueID)\n",
    "        return uniqueID\n",
    "    else:\n",
    "        uniqueIdGen(prevIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original = pd.read_csv('news_data.csv')\n",
    "#original.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "original[\"ID\"] = 'NaN'\n",
    "#`original.drop(original.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'news_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function genarates and assign the IDs to rows of dataset\n",
    "def assignID(df):\n",
    "    if(\"ID\" not in df.columns):\n",
    "        df[\"ID\"] = 'NaN' #creates a column and assign NaN to it\n",
    "        for i in df.index:\n",
    "            df[\"ID\"][i] = uniqueIdGen(df[\"ID\"].to_list())\n",
    "    elif(\"ID\" in df.columns):\n",
    "        for i in df.index:\n",
    "            df[\"ID\"][i] = uniqueIdGen(df[\"ID\"].to_list())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original.drop(original.columns[10], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#original[\"ID\"][0] = uniqueIdGen(original[\"ID\"].to_list()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a column named likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4 = df2.merge(df3, on='source.id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndFeedLikes():\n",
    "    likesDf = getLikesDf() #This returns the likes and ID df gotten from phone but hasn't been created yet. \n",
    "    \n",
    "    #this loads the data\n",
    "    original = pd.read_csv('news_data.csv')\n",
    "    original.drop(original.columns[0], axis=1) #this removes the previous unwanted indexing #But check if this works right!\n",
    "    \n",
    "    #merge the DFs\n",
    "    original.merge(likesDf, on='ID')\n",
    "    \n",
    "    #del the prev file\n",
    "    os.remove('news_data.csv')\n",
    "    \n",
    "    #create a new df thus just like overwriting it\n",
    "    original.to_csv('news_data.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelAndPredict(toPredict):\n",
    "    #convert the toPredict data into csv\n",
    "    toPredictA = jsonToCsvNormalizer(toPredict)\n",
    "    \n",
    "    #we need to filter the df thats going to be passed in the model  for prediction i.e removing unnecessary columns\n",
    "    toPredictB = dfFilter(toPredictA, ['author', 'title', 'description', 'source.name'])\n",
    "    \n",
    "    #If there's need to convert toPredict data into array so it could be fit into a clf, it should be done here\n",
    "    #####\n",
    "    \n",
    "    #loads the data\n",
    "    original = pd.read_csv('news_data.csv')\n",
    "    original.drop(original.columns[0], axis=1)\n",
    "    \n",
    "    #for now i'm going to drop the publishedAt column until we understand how to use time series data well.\n",
    "    #we would also drop the content untill we know what best to do with it\n",
    "    #url and urlToImage should also be dropped for now because they wouldnt help in training the model\n",
    "    #\n",
    "    \n",
    "    X = original[['author', 'title', 'description', 'source.name']]\n",
    "    y = original[['Likes']]\n",
    "    \n",
    "    #This function turns the dataset into the model processable data\n",
    "    readyTheData(X, y)\n",
    "    \n",
    "    #Model training\n",
    "    clf = SVC()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    #Prediction of new data\n",
    "    predLikes = clf.predict(toPredictB)\n",
    "    \n",
    "    #Removes previous/latest prediction saved in file other to pass in new prediction in future\n",
    "    if (os.path.exists('latest_prediction.csv') == True):    #---potential error\n",
    "        os.remove('latest_prediction.csv')\n",
    "        \n",
    "    #save latest prediction in file\n",
    "    predLikes.to_csv('latest_prediction.csv')\n",
    "        \n",
    "    #convert the toPredict and prediction to a new df\n",
    "    predDf = pd.Dataframe([toPredictB, predLikes], columns=original.columns) #check here for syntax error #and other errors relatedto columns\n",
    "    \n",
    "    #convert to json and save in a memory/variable for later usage #check here for syntax error\n",
    "    #predJson = predDf.to_json()\n",
    "    \n",
    "    #merge the saved data to the original data and overwrite the file #check here for syntax error\n",
    "    original = original.append(predDf)\n",
    "    original.to_csv('news_data.csv')\n",
    "    \n",
    "    #There should be a condition here to check if its not a first time user then send the data\n",
    "    \n",
    "        #send the data saved in memory to the phone \n",
    "        sendToPhone(predDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFilter(df, columnsWeWantAsList):\n",
    "    return df[columnsWeWantAsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are parts of this code that i do not understand and thus must understand it before moving further\n",
    "#It will be indicated with #---DNU -meaning do not understand\n",
    "def encodeAColumn(column):\n",
    "    for i in column.index:\n",
    "        toConvert = column[i]\n",
    "        toConvert = toConvert.encode('utf-8')\n",
    "        toConvert = int.from_bytes(toConvert, 'little') #---DNU\n",
    "        column[i] = toConvert\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeAColumn(column):\n",
    "    for i in column.index:\n",
    "        toConvert = column[i]\n",
    "        toConvert = toConvert.to_bytes((myint.bit_length() + 7) // 8, 'little')\n",
    "        toConvert = toConvert.decode('utf-8')\n",
    "        column[i] = toConvert\n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyTheData(X, y):\n",
    "    #encode some  columns int asy data for system to interprete\n",
    "    encodeAColumn(X['author'])\n",
    "    encodeAColumn(X['source.name'])\n",
    "    \n",
    "    #preprocesses and converts those columns into machine readable formats\n",
    "    columnPreprocessing(X['title'], 'title')\n",
    "    columnPreprocessing(X['description'], 'description')\n",
    "    \n",
    "    #deal with publishedAt here its a time series data\n",
    "    #for now i'm going to drop the publishedAt column until we understand how to use time series data well.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function collects the column unique contents of a df\n",
    "def collectUniqueContent(column_name):\n",
    "    listOfUniques = []\n",
    "    for i in column_name.index:\n",
    "        if(column_name[i] not in listOfUniques):\n",
    "            listOfUniques.append(column_name[i])\n",
    "    return listOfUniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should contain a creation of a dataset and a mini model to pass it to \n",
    "#Whereby we categorize the data based on if its political, sport, business and entertainment etc.\n",
    "\n",
    "#The function should collect a column and then extract all the words and turn them into a dataset excluding two and three letter words\n",
    "#Save to a file and load it back to memory \n",
    "#Then train the model using clustering model\n",
    "#Predict new data\n",
    "#increment/overwrite the dataset and file\n",
    "#overwrite the values of the column being passed with that of the labels\n",
    "\n",
    "def columnPreprocessing(column, name_of_main_column):\n",
    "    the_column = column\n",
    "    \n",
    "    #removes all the stopwords from the sentence of each row\n",
    "    the_column = removeStopwords(the_column)\n",
    "    \n",
    "    #collects and store all unique (every word ones...) words\n",
    "    unique_words = collUniqueWords(the_column)\n",
    "    \n",
    "    #turns those unique_words to columns\n",
    "    DFCreator(the_column, unique_words)\n",
    "    \n",
    "    #assess them against number of appearance each word makes in the main sentence and assign true or false i.e 1 or 0\n",
    "    assesAndAssign(the_column, name_of_main_column)\n",
    "    \n",
    "    #run the columns through a clustering model then assign the values to the  original column \n",
    "    clusterModel(the_column, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering model that does the final assignment\n",
    "def clusterModel(the_column, originalColumnName):\n",
    "    KMeans = KMeans(n_clusters = 4)\n",
    "    KMmodel = KMeans.fit(the_column)\n",
    "    labels = KMeans.labels_\n",
    "    \n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        originalColumnName[count] = i\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asseser and assigner function\n",
    "def assesAndAssign(df, nameOfMainColumn):\n",
    "    for i in df.index:\n",
    "        for word in df[nameOfMainColumn][i].split(' '):\n",
    "            if word in df.columns:\n",
    "                df[word][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns unique words into columns\n",
    "def DFCreator(init_df, listOfWords):\n",
    "    for(word in listOfWords):\n",
    "        init_df[word] = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collects all the unique words and store of each row in a column \n",
    "def collUniqueWords(column_name):\n",
    "    listOfUniques = []\n",
    "    for i in column_name.index:\n",
    "        column_words = column_name[i].split(' ')\n",
    "        for word in column_words:\n",
    "            if(word not in listOfUniques):\n",
    "                listOfUniques.append(word)\n",
    "    return listOfUniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes all the stopwords and returns the removed stopwords column\n",
    "def removeStopwords(column):\n",
    "    for i in column.index:\n",
    "        column[i] = column[i].replace(' %s '% (stopwords.words('english')), ' ')\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function sends data to the phone\n",
    "#The funtion filters the data to be sent to the phone\n",
    "#The data is sent in a json format\n",
    "def sendToPhone(predictedDataframe):\n",
    "    #create a group of df based on likes\n",
    "    predictedDataframe = predictedDataframe.groupby(predictedDataframe['Likes'])\n",
    "    likes = predictedDataframe.get_group('1') #The one could change depending on if 1 is the true or the false\n",
    "    \n",
    "    #convert the csv data thats on the memory to json format\n",
    "    #send(i.e return) the json data to the phone\n",
    "    return likes.to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
