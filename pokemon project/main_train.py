# -*- coding: utf-8 -*-
"""
Created on Wed Feb  8 06:29:20 2023

@author: SAMSUNG
"""

import torch
import numpy as np
#import initialization_file
from initialization_file import dis, gen, l_v, l_v_input, batch_size, d_optimizer, g_optimizer
from batch_file import make_batch
from loss_func import disc_loss_for_real_data, disc_loss_for_fake_data
batched_list = make_batch()

sample_images = []
num_epoch = 200
dis.train()
gen.train()
count = 399
print_every = 400
fixed_l_v = torch.from_numpy(np.random.uniform(-1, 1, size=(batch_size, l_v_input)))
for epoch in range(num_epoch):
    print('---------------------###########epoch'+str(epoch)+'##########------------------------')
    ###############################train the discriminator####################################
    for real_data in batched_list:
        count+=1 #Counter to keep track
        
        #print('total len in each epoch remaining: ', len(batched_list)-count)
        #print('len of data being processed: ', len(real_data[0]))
        
        real_data = torch.FloatTensor(real_data)
        ## Important rescaling step ## 
        #real_data = (real_data-0.5) * 2  # rescale input images from [0,1) to [-1, 1)
        #print(real_data[0])
        #print('real_data_size: ', real_data.size())
        #batch size#print(real_data.size(0))
        batch_size = real_data.size(0)
        #print('batched_size:: ',batch_size)
        
        d_optimizer.zero_grad() #clear grad
        
        #training the discriminator with real data 
        #print('....starting discriminator')
        pred_r = dis(real_data)
        #print('pred r debug')
        #print(pred_r.squeeze())
        loss_r = disc_loss_for_real_data(pred_r.squeeze(), batch_size) #pls checking for the meaning of squueze()
        with torch.no_grad():
            #print('gen inputA: ', l_v.shape)
            l_v = torch.from_numpy(np.random.uniform(-1, 1, size=(batch_size, l_v_input))) #latent vector
            fake_data = gen(l_v)
        #print(fake_data.size())
        #print(pred_f)
        #training the discriminator with fake data
        pred_f = dis(fake_data)
        loss_f = disc_loss_for_fake_data(pred_f.squeeze(), batch_size) 
        loss_d = loss_r + loss_f
        loss_d.backward()
        
        d_optimizer.step()
        
        ###############################train the generator####################################
        g_optimizer.zero_grad() #clear grad
        
        #print('..........................generating fake data................')
        #print('gen inputB: ', l_v.shape)
        l_v = torch.from_numpy(np.random.uniform(-1, 1, size=(batch_size, l_v_input))) #latent vector
        unreal_data = gen(l_v)
        dis_desc_on_g_data = dis(unreal_data)
        loss_g = disc_loss_for_real_data(dis_desc_on_g_data.squeeze(), batch_size)
        loss_g.backward()
        
        g_optimizer.step()
        
        #copied code to print some loss after some time
        # Print some loss stats
        if count == 400:
            count = 0
            # print discriminator and generator loss
            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(
                    epoch+1, num_epoch, loss_d.item(), loss_g.item()))
    #write in the code that save puts the samples generated by the generator in a variable array as well as save in a file that can be appended to
    gen.eval()
    sample = gen(fixed_l_v)
    sample_images.append(sample)
    gen.train()
    
    
